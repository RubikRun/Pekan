----------
23.08.2025
----------

Started developing the game
    Gleam House
with Pekan, in a separate repository.

----------
24.08.2025
----------

{
Found a bug in Pekan.
While rendering a grid with a checkered pattern I noticed this weird bug.
If the grid is sufficiently big (81x81 or bigger),
then the second RectangleShape being rendered
takes the color of the first RectangleShape being rendered.

Why?
The problem is this shader of Pekan:
    2D_Batch_1DTexture_FragmentShader.pkshad
Specifically, this line
    vec4 shapeColor = texture(uColorsTexture, (vShapeIndex) / float(uColorsCount - 1));
It samples the texture very close to the borders of the texels,
and for bigger values of uColorsCount, the sampled texture coordinates become too small
and maybe a floating point precision error happens?
For example, if we have
    uColorsCount = 5001
and
    vShapeIndex = 1
then this expression
    (vShapeIndex) / float(uColorsCount - 1)
evaluates to
    0.0002
However, a texel's size in this texture is
    1 / 5001 = 0.00019996
So this value 0.0002 is almost on the border between texel 0 and texel 1.
We know that since vShapeIndex is 1, we need it to sample from texel 1,
so ideally it would sample at
    (1 / 5001) + (1 / 5001) / 2
which is the center of texel 1.
So, basically we need to change this line
    vec4 shapeColor = texture(uColorsTexture, (vShapeIndex) / float(uColorsCount - 1));
to be
    vec4 shapeColor = texture(uColorsTexture, (vShapeIndex + 0.5) / float(uColorsCount));
instead.
That's it. Now it works.

All demos work as well.

Done.
}

{
Found a feature missing in Pekan and I need it for Gleam House.
We need support for custom texture coordinates for the 4 corners of our Sprite class.
Right now we always assign texture coordinates from [0, 1] to our Sprite's rectangle geometry.
We want to allow client code to determine this range.

Let's have these member variables
    glm::vec2 m_textureCoordinatesMin = { 0.0f, 0.0f };
    glm::vec2 m_textureCoordinatesMax = { 1.0f, 1.0f };
that will determine the rectangle in texture space that the sprite will map to,
instead of always mapping to the rectangle [0, 0] to [1, 1].
Then, we'll have getters and setters for them, of course.
And, most importantly, we'll use them in Sprite's
    updateVerticesWorld()
function when we set "textureCoordinates" attribute of the 4 vertices
    m_verticesWorld[0].textureCoordinates = { m_textureCoordinatesMin.x, m_textureCoordinatesMin.y };
    m_verticesWorld[1].textureCoordinates = { m_textureCoordinatesMax.x, m_textureCoordinatesMin.y };
    m_verticesWorld[2].textureCoordinates = { m_textureCoordinatesMax.x, m_textureCoordinatesMax.y };
    m_verticesWorld[3].textureCoordinates = { m_textureCoordinatesMin.x, m_textureCoordinatesMax.y };
That's it.
}

----------
31.08.2025
----------

{
To continue Gleam House's development we need a new feature in Pekan.
We need support for post-processing shaders.

For that, we'll basically have to do our rendering to a FrameBuffer object
instead of directly to the window.
This FrameBuffer object will be linked to a 2D texture
that we can then render on top of 2 triangles covering the whole window,
using a custom fragment shader, which will be the "post-processing shader".

Okay, did it.
Here's what I did:
1. Created a new class
        class FrameBuffer
    in 2 new files
        FrameBuffer.h
        FrameBuffer.cpp
    It can be created with some width and height:
        void create(int width, int height);
    It has standard destroy(), bind(), unbind() and isValid() functions.
    Internally it consists of a 2D texture and a render buffer
        Texture2D m_texture;
		RenderBuffer m_renderBuffer;
    The 2D texture will contain the color channels of rendered pixel
    and the render buffer will contain the depth and stencil channels of rendered pixels.
    What is a RenderBuffer? See in next point.
    The m_texture and m_renderBuffer are created in these 2 functions
        void createTexture(int width, int height);
		void createRenderBuffer(int width, int height);
    which are called from create(...)
    In createTexture() we create m_texture as a standard 2D texture with the same width and height as the frame buffer,
    (however we only allocate memory for it, we don't put any data, see point 3)
    and at the end we attach the texture to the frame buffer like that:
        m_texture.attachToFrameBuffer(*this);
    This attachToFrameBuffer() is a new function in class Textutre2D
        void attachToFrameBuffer(const FrameBuffer& frameBuffer) const;
    It basically needs to do this OpenGL call:
        GLCall(glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, m_id, 0));
    Similarly in createRenderBuffer() we need to create a RenderBuffer with the same width and height and attach it to the frame buffer:
        m_renderBuffer.attachToFrameBuffer(*this);
    We'll see this attachToFrameBuffer() function of class RenderBuffer below when looking at class RenderBuffer
    Finally, we have this function
        void bindTexture() const;
    which just binds the underlying texture
        m_texture.bind();
    We need this because binding the FrameBuffer and binding its texture are conceptually different,
    and users of this class WILL need to bind the texture separately.
    Also, in bind() we don't bind m_texture and we don't bind m_renderBuffer, we only bind the frame buffer.
2. Created a new class
        class RenderBuffer
    in 2 new files
        RenderBuffer.h
        RenderBuffer.cpp
    It can be created with some width and height:
        void create(int width, int height);
    It has standard destroy(), bind(), unbind() and isValid() functions.
    In the create() function, after creating the render buffer object, we need to declare its storage:
        GLCall(glRenderbufferStorage(GL_RENDERBUFFER, GL_DEPTH24_STENCIL8, width, height));
    For now, we'll hardcode it to be 24 depth bits and 8 stencil bits. Later we might abstract this away into the API.
    We need one more things, which is this function
        void attachToFrameBuffer(const FrameBuffer& frameBuffer) const;
    for attaching the render buffer to a frame buffer. It just needs to do this OpenGL call:
        glFramebufferRenderbuffer(GL_FRAMEBUFFER, GL_DEPTH_STENCIL_ATTACHMENT, GL_RENDERBUFFER, m_id);
3. We need to add a function to Texture2D
        void setSize(int width, int height, int numChannels = 4);
    that just sets the size of the texture, allocates that much memory, but doesn't set any data.
    We need this in FrameBuffer's createTexture() function to create the texture but leave its data empty.
    The data will automatically get filled because the texture is attached to the frame buffer,
    so whenever the frame buffer is bound and a draw call happens some pixel data will get filled to the texture.
4. Created a new class
        class PostProcessor
    in 2 new files
        PostProcessor.h
        PostProcessor.cpp
    The idea of this class is to tie together a FrameBuffer with a rectangle covering the whole window
    where the final texture will be rendered with the post-processing shader applied.
    Also, it will expose the API through which client code can use post-processing.
    So, it will be a singleton/static class and it will have these 3 public functions
        static bool init(const char* postProcessingShaderFilepath);
        static void beginFrame();
        static void endFrame();
    Client code will need to initialize the PostProcessor if they want to use it,
    and that's when they will provide the post-processing shader that they want to use.
    Then, in scene's render() function client code will have to call
        PostProcessor::beginFrame();
        PostProcessor::endFrame();
    if they want post-processing on that frame.
    Internally, in the .cpp file we'll have a single global instance of a FrameBuffer
        static FrameBuffer g_frameBuffer;
    We will also have a single global instance of a RenderObject
        static RenderObject g_renderObject;
    This RenderObject will be the rectangle covering the whole window,
    where g_frameBuffer's texture will be drawn and the post-processing shader will be applied.
    In PostProcessor's init() function we need to create the FrameBuffer with window's size
        const glm::ivec2 windowSize = PekanEngine::getWindow().getSize();
        g_frameBuffer.create(windowSize.x, windowSize.y);
    and then create g_renderObject as well - it's mostly straightforward, so skipping some details here,
    but the important part is that it's a rectangle covering the whole window and it has the post-processing shader.
    In PostProcessor's beginFrame() function we need to bind the frame buffer
        g_frameBuffer.bind();
    because we want any subsequent draw calls to go on this frame buffer, NOT directly on the screen.
    Then, in PostProcessor's endFrame() function we need to unbind the frame buffer
        g_frameBuffer.unbind();
    because it already contains the full rendered frame, we don't want to draw to it anymore,
    instead we want to draw to the screen now. Unbinding a frame buffer binds the default frame buffer which is the screen.
    After that we will bind frame buffer's texture which contains the rendered frame, but without post-processing yet
        g_frameBuffer.bindTexture();
    and we will render g_renderObject
        g_renderObject.render();
    which will render, on screen, the contents of the texture, which is the rendered frame, but with the post-processing shader applied.
That's it.
It almost works now.

----------
02.09.2025
----------

One last thing that we need in our post-processing is support for MSAA.
Currently when we enable MSAA in our scene (Demo06_Scene for example)
it is enabled only for the default frame buffer, which is the viewport,
however we are rendering our shapes to a separate FrameBuffer and so they are not anti-aliased.
Then, when we draw this separate FrameBuffer onto the viewport using a 2D texture
we get anti-aliasing but it's of no use because the actual shapes are already drawn on the texture.

Here's what I did to add support for MSAA in post-processing:
1. Created a new class
        Texture2DMultisample
    which is similar to Texture2D but with multiple samples per texel.
    Also it doesn't need most of the functionality of Texture2D, more specifically I removed these functions:
        setMinifyFunction, setMagnifyFunction, setWrapModeX, setWrapModeY, setBorderColor, setImage
    and the overload of create() taking in an Image.
    Then, we need this new member variable
        int m_samplesPerTexel = -1;
    that will be taken in in create()
        void create(int samplesPerTexel);
    Finally, we use
        GL_TEXTURE_2D_MULTISAMPLE
    everywhere instead of GL_TEXTURE_2D
    and in setSize() we call
        glTexImage2DMultisample(...)
    instead of glTexImage2D().
    That's about it.
2. Extend class RenderBuffer to support multiple samples.
    Add an int "samplesPerPixel" parameter to the create() function
        void create(int width, int height, int samplesPerPixel = 1);
    Then, in create() check if samplesPerPixel > 1, and if so we'll call
        glRenderbufferStorageMultisample(...)
    Otherwise we'll call
        glRenderbufferStorage(...)
    as we did until now.
    We don't need to keep this "samplesPerPixel" value in a member variable
    because we use it only once in the create() function.
    If we need it later, we'll add a member variable.
3. Extend class FrameBuffer to support multiple samples.
    Add an int "samplesPerPixel" parameter to the create() function
        void create(int width, int height, int samplesPerPixel = 1);
    together with a member variable
        int m_samplesPerPixel = -1;
    Add a new member variable
        Texture2DMultisample m_textureMultisample;
    We will use either m_texture or m_textureMultisample depending on the value of m_samplesPerPixel
    Add a new function
        void createTextureMultisample(int width, int height);
    for creating a multisample texture instead of a normal one.
    Then in create() call either createTexture() or createTextureMultisample()
    depending on the value of m_samplesPerPixel.
    Similarly in bindTexture() and destroy() use the correct texture depending on the value of m_samplesPerPixel
    Then, in
        createRenderBuffer()
    pass m_samplesPerPixel when calling m_renderBuffer's create() function:
        m_renderBuffer.create(width, height, m_samplesPerPixel);
    Finally we need this function
        void resolveMultisampleToSinglesample(FrameBuffer& targetFrameBuffer);
    that copies a multisample FrameBuffer into a single-sample one.
4. Extend class PostProcessor to support multiple samples.
    Add an int "samplesPerPixel" parameter to the init() function
        static bool init(const char* postProcessingShaderFilepath, int samplesPerPixel);
    together with a global static variable in the .cpp file
        static int g_samplesPerPixel = -1;
    Then, we'll have 2 FrameBuffers instead of one, again as global static variables
        static FrameBuffer g_frameBufferMultisample;
        static FrameBuffer g_frameBufferFinal;
    In the case of doing multisample rendering the draw calls will draw into g_frameBufferMultisample
    and in the end in endFrame() we will copy all pixel data from g_frameBufferMultisample to g_frameBufferFinal
    transforming the data from multisample to single-sample (we'll use the resolveMultisampleToSinglesample() function described above)
    In the case of doing single-sample rendering the draw calls will go directly into g_frameBufferFinal.
That's about it.

It works!

}
