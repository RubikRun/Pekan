----------
03.04.2025
----------

Officially starting work on v0.2.
Let's follow the list of features in
    plan_0004.txt

{
Let's tart with the event system. That's a big one.
I will use the following commits from Hazel Engine as reference.
    https://github.com/TheCherno/Hazel/commit/34df41651fe88041540f20df845a70642359dc17
    https://github.com/TheCherno/Hazel/commit/30516ad7109b016213eb732f14e1c7061c1db603

As a first step I will copy most of it, and just integrate it into Pekan,
to see how it works and to see if I like it.

Let's create a new directory
    Events
under
    src/PekanCore
Inside of it create the following header files
    WindowEvent.h
    Event.h
    KeyEvent.h
    MouseEvent.h
Add them to project PekanCore in the root CMake file.
Now I will copy code from the corresponding files in this Hazel commit
    https://github.com/TheCherno/Hazel/commit/34df41651fe88041540f20df845a70642359dc17
and integrate it into Pekan.
Done.

Now we have all relevant event types as classes.
Let's implement the actual event system using those classes.
The bottomline is we need to call these GLFW functions in PekanEngine
    glfwSetKeyCallback(s_window, ...);
    glfwSetCursorPosCallback(s_window, ...);
    glfwSetScrollCallback(s_window, ...);
    glfwSetMouseButtonCallback(s_window, ...);
    glfwSetWindowSizeCallback(s_window, ...);
    glfwSetWindowCloseCallback(s_window, ...);
and pass some appropriate functions as second parameters.
The functions that we will pass there will be called whenever an event of the given type occurs.

Let's create a new class
    EventHandler
in
    Event.h
and create static functions that we will use to pass as seconds parameters to the GLFW functions
    static void handleKeyEvent(GLFWwindow* window, int key, int scancode, int action, int mods);
    static void handleMouseMovedEvent(GLFWwindow* window, double xPos, double yPos);
    static void handleMouseScrolledEvent(GLFWwindow* window, double xOffset, double yOffset);
    static void handleMouseButtonEvent(GLFWwindow* window, int button, int action, int mods);
    static void handleWindowResizedEvent(GLFWwindow* window, int width, int height);
    static void handleWindowClosedEvent(GLFWwindow* window);
Then we can do this in PekanEngine.cpp
    glfwSetKeyCallback(s_window, EventHandler::handleKeyEvent);
    glfwSetCursorPosCallback(s_window, EventHandler::handleMouseMovedEvent);
    glfwSetScrollCallback(s_window, EventHandler::handleMouseScrolledEvent);
    glfwSetMouseButtonCallback(s_window, EventHandler::handleMouseButtonEvent);
    glfwSetWindowSizeCallback(s_window, EventHandler::handleWindowResizedEvent);
    glfwSetWindowCloseCallback(s_window, EventHandler::handleWindowClosedEvent);

Okay, then what? These 6 functions will be called whenever an event of their specific type occurs.
What should these functions do?
What should happen when a key is pressed, for example?
Well, we need a way for client applications to say what happens when a key is pressed.
Client applications need to be able to register a custom callback function to be called whenever a key is pressed.
Furthermore, we have more event types than just these 6 functions. We may have a couple of event types for each of these 6 functions.
For example, when ANY key event occurs this function
    handleKeyEvent()
will be called, but this may correspond either to a KeyPressedEvent or a KeyReleasedEvent.
So, client applications need to be able to register a custom callback function for each of OUR event types (8 in total).
For example, they need to be able to register a callback for when a key is pressed and another callback for when a key is released.
Let's add these 8 functions
    static inline void registerKeyPressedCallback(const KeyPressedCallback& callback) { s_keyPressedCallbacks.push_back(callback); }
    static inline void registerKeyReleasedCallback(const KeyReleasedCallback& callback) { s_keyReleasedCallbacks.push_back(callback); }
    static inline void registerMouseMovedCallback(const MouseMovedCallback& callback) { s_mouseMovedCallbacks.push_back(callback); }
    static inline void registerMouseScrolledCallback(const MouseScrolledCallback& callback) { s_mouseScrolledCallbacks.push_back(callback); }
    static inline void registerMouseButtonPressedCallback(const MouseButtonPressedCallback& callback) { s_mouseButtonPressedCallbacks.push_back(callback); }
    static inline void registerMouseButtonReleasedCallback(const MouseButtonReleasedCallback& callback) { s_mouseButtonReleasedCallbacks.push_back(callback); }
    static inline void registerWindowResizedCallback(const WindowResizedCallback& callback) { s_windowResizedCallbacks.push_back(callback); }
    static inline void registerWindowClosedCallback(const WindowClosedCallback& callback) { s_windowClosedCallbacks.push_back(callback); }
where
    typedef std::function<bool(KeyPressedEvent&)> KeyPressedCallback;
    typedef std::function<bool(KeyReleasedEvent&)> KeyReleasedCallback;
    typedef std::function<bool(MouseMovedEvent&)> MouseMovedCallback;
    typedef std::function<bool(MouseScrolledEvent&)> MouseScrolledCallback;
    typedef std::function<bool(MouseButtonPressedEvent&)> MouseButtonPressedCallback;
    typedef std::function<bool(MouseButtonReleasedEvent&)> MouseButtonReleasedCallback;
    typedef std::function<bool(WindowResizedEvent&)> WindowResizedCallback;
    typedef std::function<bool(WindowClosedEvent&)> WindowClosedCallback;
And what will these 8 functions do?
They will just add the given callback to a list of callbacks corresponding to the specific event type.
So we need 8 lists of callbacks, as members in EventHandler
    static std::vector<KeyPressedCallback> s_keyPressedCallbacks;
    static std::vector<KeyReleasedCallback> s_keyReleasedCallbacks;
    static std::vector<MouseMovedCallback> s_mouseMovedCallbacks;
    static std::vector<MouseScrolledCallback> s_mouseScrolledCallbacks;
    static std::vector<MouseButtonPressedCallback> s_mouseButtonPressedCallbacks;
    static std::vector<MouseButtonReleasedCallback> s_mouseButtonReleasedCallbacks;
    static std::vector<WindowResizedCallback> s_windowResizedCallbacks;
    static std::vector<WindowClosedCallback> s_windowClosedCallbacks;
and each of the register*() functions will just add the callback to the corresponding list, as shown above.
Then, once we have a list of callbacks, we can think about what the handle*() functions should do.
Well, they should call the callbacks.
But to do that, they need to first create an event.
For example, the handleMouseMovedEvent() function needs to construct a MouseMovedEvent, like that
    MouseMovedEvent event = { float(xPos), float(yPos) };
and then it can call the callbacks registered for MouseMovedEvents (the callbacks in s_mouseMovedCallbacks) like that
    for (const MouseMovedCallback& callback : s_mouseMovedCallbacks)
    {
        if (callback(event))
        {
            event.m_handled = true;
            break;
        }
    }
(The logic behind the if statement and setting m_handled is explained below, for now only notice that we're calling callback(event))
Constructing the event of the specific type will be different in the different handle*() functions,
but the for-loop for calling the callbacks will be exactly the same,
so we can create a template function for that
    template <typename EventT, typename CallbackT>
    static void handleEvent(EventT& event, const std::vector<CallbackT>& callbacks)
    {
        for (const CallbackT& callback : callbacks)
        {
            if (callback(event))
            {
                event.m_handled = true;
                break;
            }
        }
    }
Now the implementation of handleMouseMovedEvent() boils down to just this
    void EventHandler::handleMouseMovedEvent(GLFWwindow* window, double xPos, double yPos)
    {
        MouseMovedEvent event = { float(xPos), float(yPos) };
        handleEvent(event, s_mouseMovedCallbacks);
    }
Neat!
It's a bit more complicated in the handle*() functions that need to differentiate between multiple different event types.
For example handleKeyEvent() may construct either a KeyPressedEvent or a KeyReleasedEvent() depending on the value of the "action" parameter.
Those details are not important, and quite straightforward, so leaving it at that.

One final thing to explain here is why don't we just call
    callback(event)
in the handleEvent() template function.
Why do we do
    if (callback(event))
    {
        event.m_handled = true;
        break;
    }
All callbacks here return a bool which means whether the event was handled, or just skipped.
This might make more sense especially when considering multiple layers.
For example if we have a game where clicking the mouse makes the player jump,
this game world will be on one layer, but on top of that we may have another layer with GUI,
where clicking the mouse has another meaning.
So in that case we would have 2 callbacks registered in s_keyPressedCallbacks
and if one of them handles the event we want to NOT call the callback of the other layer.

That's about it. That's the event system.

Let's test it in Demo03.
Let's create callback functions for each event type, in Snake.cpp
    bool onKeyPressed(Pekan::KeyPressedEvent& event);
    bool onKeyReleased(Pekan::KeyReleasedEvent& event);
    bool onMouseMoved(Pekan::MouseMovedEvent& event);
    bool onMouseScrolled(Pekan::MouseScrolledEvent& event);
    bool onMouseButtonPressed(Pekan::MouseButtonPressedEvent& event);
    bool onMouseButtonReleased(Pekan::MouseButtonReleasedEvent& event);
    bool onWindowResized(Pekan::WindowResizedEvent& event);
    bool onWindowClosed(Pekan::WindowClosedEvent& event);
and let's implement them to just log the event using PekanLogger
    bool Snake::onKeyPressed(KeyPressedEvent& event)
    {
        PK_LOG_INFO(event, "Boris");
        return true;
    }
    ...
We need to register these callbacks using the EventHandler. We can do that in Snake's create() function, like that
    EventHandler::registerKeyPressedCallback(std::bind(&Snake::onKeyPressed, this, std::placeholders::_1));
    EventHandler::registerKeyReleasedCallback(std::bind(&Snake::onKeyReleased, this, std::placeholders::_1));
    EventHandler::registerMouseMovedCallback(std::bind(&Snake::onMouseMoved, this, std::placeholders::_1));
    EventHandler::registerMouseScrolledCallback(std::bind(&Snake::onMouseScrolled, this, std::placeholders::_1));
    EventHandler::registerMouseButtonPressedCallback(std::bind(&Snake::onMouseButtonPressed, this, std::placeholders::_1));
    EventHandler::registerMouseButtonReleasedCallback(std::bind(&Snake::onMouseButtonReleased, this, std::placeholders::_1));
    EventHandler::registerWindowResizedCallback(std::bind(&Snake::onWindowResized, this, std::placeholders::_1));
    EventHandler::registerWindowClosedCallback(std::bind(&Snake::onWindowClosed, this, std::placeholders::_1));
Okay, it works!
Pretty nice.

Final note, I copied only the event classes from Hazel Engine, the EventHandler was my own creation.

Event system done.
}

----------
07.04.2025
----------

{
Now that we have the event system, it'd be good to also allow applications to ask for the current state of things,
for example where the mouse is currently, or what the resolution is currently, or is a given key pressed at the moment.
I wouldn't call this a "part" of the event system, but it's definitely related, and more like a complementary feature.

So what we need basically, is something similar to these 4 temporary functions that we have right now in PekanEngine
    static bool isKeyPressed_W();
    static bool isKeyPressed_A();
    static bool isKeyPressed_S();
    static bool isKeyPressed_D();
I made those functions to allow the snake game (Demo03) to work before having the event system.
We need similar functions now, but done better, and covering everything that the event system covers.

Let's start with key events - KeyPressedEvent and KeyReleasedEvent.
We can cover those with 3 functions
    static bool isKeyPressed(int key);
    static bool isKeyReleased(int key);
    static bool isKeyRepeating(int key);
Next type of event to cover is MouseMovedEvent.
We can cover that with 1 function
    static glm::vec2 getMousePosition();
Next is MouseScrolledEvent - we will not cover that because it makes no sense to ask for the current mouse scroll
(and because GLFW doesn't support it)
Next is MouseButtonPressed and MouseButtonReleased. We need 2 functions
    static bool isMouseButtonPressed(bool leftOrRight);
    static bool isMouseButtonReleased(bool leftOrRight);
And I think the final one is WindowResizedEvent. We need 1 function
    static glm::ivec2 getWindowResolution();
That's it.
Done.
}

----------
11.04.2025
----------

{
Next feature to implement is layers.
I will use as reference this commit from Hazel Engine
    https://github.com/TheCherno/Hazel/commit/5bd809312a266c23d13d84dcd08a833a526aa264

Ended up not using much, I only copied and changed Layer and LayerStack,
everything else I had to do on my own.

I did the whole thing at once, without taking notes, so here's a sum up:
I'll divide it into 3 parts - layers, init/exit/update/render layers, events in layers.

1. First of all, a layer fundamentally is just something that can be initialized, exited, updated and rendered
    virtual bool init() { return true; }
    virtual void exit() {}
    virtual void update() {}
    virtual void render() {}
It also has a name but that's not important
    std::string m_name;
Then we have a LayerStack which is just a wrapper over a list of layers
    std::vector<Layer*> m_layers;
and it provides iterators for traversing the layer stack forwards and backwards
    std::vector<Layer*>::iterator begin() { return m_layers.begin(); }
    std::vector<Layer*>::iterator end() { return m_layers.end(); }
    std::vector<Layer*>::reverse_iterator rbegin() { return m_layers.rbegin(); }
    std::vector<Layer*>::reverse_iterator rend() { return m_layers.rend(); }
Nothing interesting there.
So how are we going to use layers in Pekan?
Well, scenes and GUI windows will be layers.
We actually don't need the class
    PekanScene
at all, because it's literally just a Layer without any extra functionality.
We still need the class
    PekanGUIWindow
because it has some ImGui-specific functionality in render(), but other than that we'll make it derive from Layer
    class PekanGUIWindow : public Layer

2. Okay, so scenes and GUI windows are layers.
Now what?
Well, currently in PekanApplication we have 2 members for a scene and a GUI window.
We don't want that. We want clients to be able to compose scenes and GUI windows as they wish,
maybe they want multiple scenes on top of each other, or they don't want a GUI window, or they want multiple GUI windows, etc.
So instead of having 2 members for a scene and a GUI window let's have a single LayerStack member
    LayerStack m_layerStack;
and then client applications can configure this stack as they wish in the _init() function, for example like that
    m_layerStack.pushLayer(demoScene);
    m_layerStack.pushLayer(demoGuiWindow);
(Notice that the order of adding the layers is important. Layers added first will be rendered first)
Then the only thing left to do is make PekanApplication work with this layer stack instead of a scene and a GUI window,
so in PekanApplication.cpp in all places where we previously did something with m_scene or m_guiWindow, (init/exit/update/render)
now we need to do it with all layers in the layer stack, like that for example:
    // Update all layers
    for (Layer* layer : m_layerStack)
    {
        if (layer)
        {
            layer->update();
        }
    }
    // Render all layers
    for (Layer* layer : m_layerStack)
    {
        if (layer)
        {
            layer->render();
        }
    }
And finally, we need to update our demos a bit, to configure the layer stack in their _init() functions.
That's it.
Now everything works same as before, but demos can configure layers freely.

3. Trickies part here is making events work with layers.
Currently we have a static class EventHandler and any code can register a callback for any type of event,
which is ok, but it gives too much freedom to clients.
We want to enforce the behavior of events being propagated through layers
in the opposite order of rendering, so that layers drawn last receive events first, because they are on top.
What we want is a way for each layer to specify its own way of handling a given type of event,
and then Pekan will automatically send events to layers in the correct order
and stop sending once a layer has successfully handled an event.
How can we do that?
Well, first we can have virtual functions for each type of event inside of class Layer
    virtual bool onKeyPressed(KeyPressedEvent& event) { return false; }
    virtual bool onKeyReleased(KeyReleasedEvent& event) { return false; }
    virtual bool onMouseMoved(MouseMovedEvent& event) { return false; }
    virtual bool onMouseScrolled(MouseScrolledEvent& event) { return false; }
    virtual bool onMouseButtonPressed(MouseButtonPressedEvent& event) { return false; }
    virtual bool onMouseButtonReleased(MouseButtonReleasedEvent& event) { return false; }
    virtual bool onWindowResized(WindowResizedEvent& event) { return false; }
    virtual bool onWindowClosed(WindowClosedEvent& event) { return false; }
so that derived classes can choose to override some of these functions if they want to react to a specific event.
Then we have to actually call those functions on each layer of the layer stack when an event occurs,
and we have to pass an Event object of the corresponding subtype.
How can we do that?
Well, we already have these 6 static functions in EventHandler
    static void handleKeyEvent(GLFWwindow* window, int key, int scancode, int action, int mods);
    static void handleMouseMovedEvent(GLFWwindow* window, double xPos, double yPos);
    static void handleMouseScrolledEvent(GLFWwindow* window, double xOffset, double yOffset);
    static void handleMouseButtonEvent(GLFWwindow* window, int button, int action, int mods);
    static void handleWindowResizedEvent(GLFWwindow* window, int width, int height);
    static void handleWindowClosedEvent(GLFWwindow* window);
They are directly linked to GLFW by PekanEngine like that:
    glfwSetKeyCallback(s_window, EventHandler::handleKeyEvent);
    glfwSetCursorPosCallback(s_window, EventHandler::handleMouseMovedEvent);
    glfwSetScrollCallback(s_window, EventHandler::handleMouseScrolledEvent);
    glfwSetMouseButtonCallback(s_window, EventHandler::handleMouseButtonEvent);
    glfwSetWindowSizeCallback(s_window, EventHandler::handleWindowResizedEvent);
    glfwSetWindowCloseCallback(s_window, EventHandler::handleWindowClosedEvent);
They construct an Event object of a specific type and call the registered callbacks passing that Event object.
Now we still need to construct the Event objects in the exact same way,
but instead of calling the registered callbacks,
we will call the on*() functions on each layer of the layer stack.
Problem is we don't have access to the layer stack because this EventHandler is all static,
and the layer stack is part of an instance of PekanApplication,
so we'll need to move these handle*() functions to be member functions of PekanApplication
    void handleKeyEvent(int key, int scancode, int action, int mods);
    void handleMouseMovedEvent(double xPos, double yPos);
    void handleMouseScrolledEvent(double xOffset, double yOffset);
    void handleMouseButtonEvent(int button, int action, int mods);
    void handleWindowResizedEvent(int width, int height);
    void handleWindowClosedEvent();
removing the GLFWwindow pointer (we'll see why in a minute)
To implement them in PekanApplication we'll construct the Event objects in the exact same way,
but then instead of calling EventHandler's template function handleEvent()
which works with the registered callbacks,
we'll create a new template function
    static void _dispatchEvent(EventT& event, LayerStack& layerStack, bool (Layer::*onEventFunc)(EventT&))
that calls the on*() functions of each layer of the layer stack until the event is handled.
Then we just call that function with the constructed Event object and give it our application's layer stack
and a pointer to the specific on*() function that it needs to call on the layers. For example, like that:
    MouseMovedEvent event = { float(xPos), float(yPos) };
    _dispatchEvent(event, m_layerStack, &Layer::onMouseMoved);
That's it, that's what the handle*() functions do.
Now the only thing left to do is to connect those handle*() functions to GLFW
and actually make it so that they are called when an event occurs.
We cannot directly connect them to GLFW with the glfwSet*Callback() functions
because they require a callback which is a global/static function
(our handle*() functions are member functions of PekanApplication and so they contain an extra implicit "this" parameter)
What we can do is create yet another set of functions, but this time static in PekanEngine
    static void keyCallback(GLFWwindow* window, int key, int scancode, int action, int mods);
    static void mouseMovedCallback(GLFWwindow* window, double xPos, double yPos);
    static void mouseScrolledCallback(GLFWwindow* window, double xOffset, double yOffset);
    static void mouseButtonCallback(GLFWwindow* window, int button, int action, int mods);
    static void windowResizedCallback(GLFWwindow* window, int width, int height);
    static void windowClosedCallback(GLFWwindow* window);
Now these functions have the exact signature required by the glfwSet*Callback() functions
so we can directly register them to GLFW, like that
    glfwSetKeyCallback(s_window, keyCallback);
    glfwSetCursorPosCallback(s_window, mouseMovedCallback);
    glfwSetScrollCallback(s_window, mouseScrolledCallback);
    glfwSetMouseButtonCallback(s_window, mouseButtonCallback);
    glfwSetWindowSizeCallback(s_window, windowResizedCallback);
    glfwSetWindowCloseCallback(s_window, windowClosedCallback);
And now we reach the root of our problem.
How can we make these static functions call PekanApplication's handle*() functions that are very much non-static,
they are member functions and require a specific instance of a PekanApplication.
Well, we need a static instance of PekanApplication.
We need a mechanism in PekanEngine for a PekanApplication to be "registered"
and kept under a static member variable
    static PekanApplication* s_application;
For that we'll add this new function
    static void registerApplication(PekanApplication* application);
that every PekanApplication will need to call to "register" itself in PekanEngine
before it could be run.
And finally, having this static s_application, we can implement the callbacks as follows:
    void PekanEngine::keyCallback(GLFWwindow* window, int key, int scancode, int action, int mods)
    {
        if (s_application)
        {
            s_application->handleKeyEvent(key, scancode, action, mods);
        }
    }
    ...
That's it.
To sum up, for example looking at a mouse moved event:
Mouse moves -> GLFW calls PekanEngine::mouseMovedCallback()
-> it calls PekanApplication::handleMouseMovedEvent()
-> it constructs a MouseMovedEvent object and calls _dispatchEvent() with it
-> it calls Layer::onMouseMoved() on each layer of the layer stack until the event is handled
-> Layer::onMouseMoved() can be implemented by client to do whatever they want and return true if event is handled
   If not implemented, it will not do anything and just return false, meaning continue to next layer
}

----------
15.04.2025
----------

{
Next feature we'll implement from the list is a simple one:
    Create a way for applications to specify the window's title

Taking a quick look at our current architecture it makes the most sense
that we let applications have a name and then Pekan will automatically set window's title to be that name.
Each specific application will be able to set its name inside of the _init() method,
similar to how it configures the layer stack.
So, to do that, just add this member variable in PekanApplication
    std::string m_name;
and then we can set its value inside of Demo03_Application::_init() like that
    m_name = "Snake Game";
Same for the other demos as well.

Okay, now each application has a name.
We need to use that name for the window's title.
Window's title is set in PekanEngine.cpp like that
    s_window = glfwCreateWindow(width, height, DEFAULT_WINDOW_TITLE, nullptr, nullptr);
So instead of using the constant DEFAULT_WINDOW_TITLE we can use the registered application
    s_application
and use its name.
For that we'll need to have a getter for the name in PekanApplication
    const std::string& getName() const { return m_name; }

Hm but now there's a problem.
The function creating the window is
    PekanEngine::createWindow()
which is called by
    PekanEngine::init()
which is called in
    PekanApplication::init()
before registering the application, so when creating the window we don't yet know the name of our application.
How can we solve this?
We could separate engine initialization from window creation so that PekanApplication
can first initialize the engine, then register itself, and then use the engine to create a window.
But actually a better idea, at least at this point, seems to be merging registering with initialization,
so instead of having a separate function registerApplication() we can just add a parameter to
    PekanEngine::init()
which will be the application. Then init() will handle everything in the correct order.
Let's do that.

Okay, that solved the problem, but now there is another problem of similar nature.
We want to use the member PekanApplication::m_name when calling PekanEngine::init()
but this happens before calling PekanApplication::_init() so the specific application
has not set its name yet.
What we can do is to have a virtual getter for the name that should be implemented by specific applications
if they want to have a name.
So remove the member m_name and change getName() to be
    virtual std::string getName() const { return ""; }
Now in Demo03_Application instead of setting m_name = "Snake Game" inside of _init()
we can implement the getName() function, like that
    std::string getName() const override { return "Snake Game"; }
That's it. Now when PekanEngine calls
    s_application->getName();
this will call the overriden version of getName() and it will return "Snake Game".
If there is no overriden version of getName() then it will call the base version, PekanApplication's version,
which just returns an empty string, and the window's title will default to
    DEFAULT_WINDOW_TITLE

Okay, done.
Looks good. Every application has its name, and window's title corresponds to that name.
}

----------
16.04.2025
----------

{
Next feature we'll look at from the list is also fairly simple (HAHA from future me):
    Limit FPS
Currently PekanEngine enables VSync by default, so FPS is automatically set to be equal to the display's FPS.
That's a good default behavior but we want to allow applications to specify custom FPS.
For that, let's add a member
    double m_fps = 0.0;
to PekanApplication, together with a protected setter
    inline void setFPS(double fps) { m_fps = fps; }
If noone sets FPS it will stay at 0.0 which will mean use VSync.
We will also remove the line that enables VSync in PekanEngine
    glfwSwapInterval(1);
Instead we will add these 2 new functions to PekanEngine
    static void enableVSync();
	static void disableVSync();
for enabling/disabling VSync.
Then in PekanApplication's run() function, before the main loop, we want to enable VSync if there is no FPS specified
    const bool useFPS = (m_fps > 0.0);
    if (!useFPS)
    {
        PekanEngine::enableVSync();
    }
If there is an FPS specified we will need to manually limit our frames to that FPS.
How?
There are a couple of approaches, it's not that straightforward.
We'll add a new class FpsLimiter which will implement the waiting functionality
such that we reach the desired FPS.

I played around with different methods of implementing FpsLimiter. I liked 2 of them.
Couldn't choose between them. I think there's a use case for both, so I'll keep them both.
I called them "Sleep Compensate" and "Wait Blocking".
At the top of FpsLimiter.h you can see a description of what they do and what their pros and cons are.

I decided it will be bad for performance to do any runtime checks for choosing between the 2 implementations
(for example an environment variable, or a member variable in PekanApplication that can be set by derived classes)
So instead I decided to just use a macro for choosing between the 2 implementations at compile time
    #define PEKAN_FPS_LIMITER_IMPL_SLEEP_COMPENSATE 1
    #define PEKAN_FPS_LIMITER_IMPL_WAIT_BLOCKING 0
We'll see later if we need to ship Pekan with both options included.
For now I just wanted to have them both in code, but to use only one at a time.

Looks good. Tested it quite a bit.
Done.
}

----------
17.04.2025
----------

{
I'd like to look at this TO-DO item now:
    Provide a way for applications to specify their way of closing

Currently we are doing this directly inside of PekanApplication's main loop
    // Close window if escape key is pressed
    if (glfwGetKey(window, GLFW_KEY_ESCAPE) == GLFW_PRESS)
    {
        glfwSetWindowShouldClose(window, true);
    }
which is not good. We don't want every application to be closable with the escape key.
We want every application to be able to implement its own closing logic.
So let's remove this from PekanApplication's main loop.
The only demo that needs to be closable by anything other than the X button of the window, is Demo03.
It needs to be closable with the escape key.

Well, we have the event system now so we can definitely use that.
We can make Demo03_Scene react to pressing the escape key, like that
    bool onKeyPressed(Pekan::KeyPressedEvent& event) override;    (Demo03_Scene.h)
and implementing it like
    bool Demo03_Scene::onKeyPressed(Pekan::KeyPressedEvent& event)    (Demo03_Scene.cpp)
    {
        if (event.getKeyCode() == GLFW_KEY_ESCAPE)
        {
            // HERE WE HAVE TO MAKE THE APPLICATION CLOSE
            return true;
        }
        return false;
    }
Okay, that works. It is activated when the escape key is pressed.

Now how do we make the application close? Until now in PekanApplication we did this
    glfwSetWindowShouldClose(window, true);
Obiously we cannot do that from Demo03's code.
PekanApplication should expose an API for closing the application, for example like that
    void stopRunning();    (PekanApplication.h)
and implementing it like
    void PekanApplication::stopRunning()    (PekanApplication.cpp)
    {
        glfwSetWindowShouldClose(PekanEngine::getWindow(), true);
    }
but it'd be good for that API to not be public.
We don't want any code to be able to close an application, we need more control than that.
At first sight it looks like we can just make this new function stopRunning() be protected.
That way only derived applications will be able to call it.
But that's not exactly what we want because it's not the application's logic that will want to close the application,
instead it's the layer's logic. We cannot react to events inside of Demo03_Application,
we can react to events inside of Demo03_Scene.
So we want a layer to be able to close its application.
That's a bit problematic with our current architecture because layers are not aware that an application contains them,
they don't know which one their application is.
Is it a good idea to allow each layer to know its parent application?
Not 100% sure but it seems so. I cannot think of a better solution at the moment,
without complicating things way too much, or changing architecture overall.
So let's go with that.
Let's make every layer know its parent application, by adding a member
    PekanApplication* m_application = nullptr;
to class Layer and let's require an application parameter in Layer's constructor
    Layer(const std::string& name, PekanApplication* application) : m_name(name), m_application(application) {}
Now we need to change a few things, we need to add this extra application parameter to a bunch of other constructors,
but that's okay, easy.

Okay, now every layer knows its parent application.
So it looks like we can make class Layer be a friend to class PekanApplication
    friend class Layer;
and then this function stopRunning() can be called from class Layer.
However, there's a problem. It cannot be called from classes derived from Layer,
and that was the whole point. We want derived classes of Layer to be able to close their application.
It turns out friendship in C++ is not inheritable, so the fact that Layer is a friend to PekanApplication
does NOT mean that derived classes of Layer will be friends to PekanApplication.
There is no way to make derived classes of Layer be friends of PekanApplication.
What we can do instead is to create a protected wrapper API in Layer
    protected:
        void stopRunningApplication();
which calls m_application's stopRunning() function
    if (m_application)
    {
        m_application->stopRunning();
    }
This code is okay because it's part of Layer, which is a friend to PekanApplication.
Then derived classes cannot directly call m_application->stopRunning()
but they can call this protected function stopRunningApplication().
So now we can finally implement Demo03_Scene's closing logic like that
    bool Demo03_Scene::onKeyPressed(Pekan::KeyPressedEvent& event)
    {
        if (event.getKeyCode() == GLFW_KEY_ESCAPE)
        {
            stopRunningApplication();
            return true;
        }
        return false;
    }
Works perfectly.
Done.
}

----------
19.04.2025
----------

{
Now let's look at this TO-DO item:
    Create an ASSERT() mechanism

Did some research on what are common ways of implementing an ASSERT() macro.
Created this new macro
    PK_ASSERT(CND, MSG, SND)
in
    PekanLogger.h
It takes in a condition, a message and a sender.
Message and sender have the exact same meaning as in PK_LOG_ERROR() and the other log macros.
Condition is a condition to be checked - if it's false only then the message should be shown,
and code should break on this line.
This should happen only in debug builds.
In non-debug builds the condition should just be executed as normal code.

To do this properly I added this new function in PekanLogger.h
    void _logAssertToConsole(const char* msg, const char* sender, const char* condition);
and implemented it in PekanLogger.cpp

That's about it. Not much to say.

Then I went through all places where PK_LOG_*() macros are used
and I changed some of them to PK_ASSERT().
Tested all demos. Tested how a failing assertion looks like.
Looks good. Done.
}

----------
21.04.2025
----------

{
Now let's look at this TO-DO item:
    Create a Window class

It's fairly straightforward. Mostly just refactored code from PekanEngine into this new class Window.
One thing worth mentioning is these 6 static functions in Window
    static void keyCallback(GLFWwindow* window, int key, int scancode, int action, int mods);
    static void mouseMovedCallback(GLFWwindow* window, double xPos, double yPos);
    static void mouseScrolledCallback(GLFWwindow* window, double xOffset, double yOffset);
    static void mouseButtonCallback(GLFWwindow* window, int button, int action, int mods);
    static void windowResizedCallback(GLFWwindow* window, int width, int height);
    static void windowClosedCallback(GLFWwindow* window);
They are registered as the 6 event callbacks in GLFW, using this function in Window
    void setEventCallbacks();
Exactly the same as it was in PekanEngine, but now class Window is not static,
so it kind of doesn't make sense that we register these static functions as event callbacks.
This is a temporary situation that we're in, because currently we support only 1 window and 1 application at a time.
In the future we must think about 1 application running across multiple windows
and then being able to have different event callbacks for each window.
For now we'll stick with having a single static set of event callbacks and having a single window.
That's just a future note.
That said, everything works as before, we just have window logic cleanly refactored into a Window class.
Done.
}

----------
25.04.2025
----------

{
I noticed a bug.
Setting these 2 member variables like that in a demo application's _init() function
    m_isFullScreen = true;
    m_shouldHideCursor = true;
doesn't work. Of course it doesn't because in PekanApplication::init() we are calling
    PekanEngine::init(this, m_isFullScreen, m_shouldHideCursor)
before calling demo application's _init().

Something is fundamentally wrong with Pekan's architecture there.
Trying to put it into words:
- A specific application should be able to configure window's properties at the init stage
- Creating a window should happen after that, because it needs to know the properties
- Loading OpenGL and initializing ImGui require a window (a GLFW context)
There are 2 options that I can think of
1. Separate window creation from engine initialization, so that PekanApplication can first initializa the engine,
   then call specific application's _init() function, and then create window by calling some function on PekanEngine.
2. Create a virtual function in PekanApplication that returns window's properties and can be overriden
   on specific applications to return the properties needed by the specific application.
   Then PekanEngine will call that function in its own init() function.
I will go with option 1 because I feel like it will be better for our future architecture.
Option 2 seems cleaner at this stage because we always have a single window,
but in the future I want Pekan to support multiple windows,
and also to work without a window at all, for example for console games or other types of mediums.

I did option 2.
More specifically, added a function
    static bool createWindow(WindowProperties properties);
in PekanEngine, and removed the 2 window-related parameters from PekanEngine::init(), now it's just this:
    static bool init(PekanApplication* application);
and the only thing it does currently is register the application.
Creating the window, loading OpenGL and initializing ImGui is done in createWindow().
Then specific applications should call createWindow(), if they want a window at all,
and they can configure the window by passing a WindowProperties object.
Additionally, now that engine initialization is separated from window creation,
it'd be good to have another member isWindowCreated, similar to isInitialized
    static bool isInitialized;
    static bool isWindowCreated;

By the way, I did set FPS to 60 in Demo03 and Demo02, because they are moving faster on my laptop display (higher refresh rate).
Should've done this together with the FPS commit earlier, but whatever, doing it now.

Bug is fixed now, architecture is better, and applications have cleaner and more flexible control over the window.
Done.
}

{
Now let's tackle this annoying TO-DO item
    Create Pekan enums corresponding to event-related enums in GLFW, like GLFW_KEY_W

Okay, it was not that annoying.
Created this new header file
    KeyEvent_Enums.h
containing this new enum class
    enum class KeyCode
that holds all key codes, with their numerical value matching that of GLFW key codes,
so that you can just cast a KeyCode enum into an int and use it with GLFW.

Similarly added this new header file
    MouseEvent_Enums.h
containing this new enum class
    enum class MouseButton
that holds just the values LeftButton and RightButton.
Later, we might need other mouse buttons, but for now we're good with just these 2.

Then we just include those headers in a couple of files, like KeyEvent.h and PekanEngine.h
and we make the key-related and mouse-button-related functions take in a KeyCode or a MouseButton instead of an int.

That's about it.
Everything works as before, but now we don't need to use GLFW enums in demo applications, we can use the new Pekan enums.
Done.
}

----------
02.05.2025
----------

{
Time to work on this TO-DO item
    Create an event queue so that events can be handled together at once, at a specific stage of the main loop.
Seems hard.

I considered a few different decisions along the way, argued with ChatGPT for a while :D
At the end what I did is fairly simple:

Created this new class in Event.h
    EventQueue
that is basically a wrapper over
    std::queue<std::unique_ptr<Event>>
a queue of base Event pointers.

Added this new member to PekanApplication
    EventQueue m_eventQueue;
Now each application has an event queue.
The idea is: Events that are not handled by any layer will be pushed to the event queue
and they can be processed all at once by a custom logic.
The custom logic of how to process the event queue will be this new virtual function in PekanApplication
    virtual void handleEventQueue()
Its default behavior is just to pop all events out of the queue.
A derived application can chose to override this function
and it will be called by PekanApplication's run() function, each frame.

That's about it. The rest is implementation details.

An architecture decision that I considered is if I should allow each layer to specify custom event queue handling logic,
so basically the virtual function
    virtual void handleEventQueue()
could be part of class Layer instead of class PekanApplication,
and then PekanApplication could call this function on each layer.
However, this leaves too much freedom to layers, because each layer can do whatever it wants with the event queue.
That's why I decided NOT to do it like that, and instead have a single event queue handling function inside of PekanApplication.
In the future it might make sense to do this per-layer thing, because we might want to allow certain layers
to handle certain events from the event queue and other layers to handle other events,
but for now I'm not too sure if it'd be needed, and it complicated things way too much.

Done.
}

----------
04.05.2025
----------

{
Next TO-DO item that I will implement is this
    Create a delta-time mechanism for FPS-independent movement

As an example I want the cube in Demo02 to rotate at the same speed no matter the FPS.

Okay, that was pretty easy.
Added this new class
    DeltaTimer
in 2 new files
    DeltaTimer.h
    DeltaTimer.cpp
under this new directory
    Time
in PekanCore, where I also put
    FpsLimiter.h
    FpsLimiter.cpp

The new class DeltaTimer is quite simple.
It has just 1 function
    double getDeltaTime();
which returns the "delta time", meaning the time that has passed since this function was last called.
If the function is called for the first time it will just return 0.0.
Imlpementation is straightforward.

Then how do we use this DeltaTimer class?
Well, we add this new member variable in PekanApplication
    DeltaTimer m_deltaTimer;
and then just use it in the run() function to get the delta time
    const double deltaTime = m_deltaTimer.getDeltaTime();
inside the main loop just before calling update() on all layers.
This deltaTime variable now contains the time passed since last frame.
We can pass this deltaTime to the update() function of all layers.
So we just add it as an extra parameter to Layer::update()
    virtual void update(double deltaTime) {}
Now each layer can access this deltaTime parameter in their update() function
and can use it to multiply their movement speeds, to achieve FPS-independent movement.

In Demo02 that would look like:
    m_rotation += float(60.0 * dt);
Instead of adding a constant (it was 1.0) we can add a constant times dt.
Now whatever FPS we use for Demo02, the cube will move with the same speed.

Done.
}

{
Next is this TO-DO item
    Create a way for derived applications to set up layer stack without directly touching the member variable m_layerStack...

Just added this output parameter
    LayerStack& layerStack
to the _init() function in PekanApplication
    virtual bool _init(LayerStack& layerStack) = 0;
Now derived applications can touch the layer stack only through this variable, only in this function.
Made the member variable
    m_layerStack
be private, together with all other member variables in PekanApplication.
Previously
    m_fps
needed to be protected because derived applications used to set m_fps directly inside of _init()
but now we have setFPS() for that, so m_fps doesn't need to be protected anymore, it should be private.

Done.
}

{
We have one final TO-DO item
    Use PekanEngine's event polling on these 2 lines in PekanApplication.cpp
        if (glfwGetKey(window, GLFW_KEY_ESCAPE) == GLFW_PRESS)
        glfwGetFramebufferSize(window, &windowWidth, &windowHeight);

This is actually already done.
First line doesn't exist anymore because we support custom closing logic and not always closing with escape key.
Second line has been changed to use Pekan's event polling
    const glm::ivec2 frameBufferSize = window.getFrameBufferSize();

Done.
}

{
All TO-DO items are done now.
We have 4 more features to implement, and 1 feature (layering) to test and maybe finish.

Let's start implementing
    RenderObject wrapper

----------
15.05.2025
----------

While implementing the RenderObject wrapper I needed to use PK_ASSERT a lot,
and I wanted to use it without a message or a sender, just to quickly check some condition.
Currently the PK_ASSERT macro requires 3 parameters - condition, message, sender.
Let's create a lighter version of PK_ASSERT, call it
    PK_ASSERT_QUICK
It's the same but with the condition only.

Okay, done implementing RenderObject wrapper. It's now under
    src/PekanRenderer/RenderObject.h
    src/PekanRenderer/RenderObject.cpp
I mostly copied it from Demo03, and changed all demos to use it.
One thing worth mentioning:
I added 2 overloads of the create() function, one without index data, and one without vertex data and index data.
Both of these overloads still create the vertex buffer and the index buffer but they create them empty.
Later client can fill vertex data and/or index data using the setVertexData() and setIndexData() functions.

All demos work as before. RenderObject's API looks good.
Done.
}

{
While we're at it let's do this new TO-DO item
    In RenderObject's setVertexData() and setIndexData() functions do something so that client doesn't have to provide BufferDataUsage each time.
    It should be optional.
    If they don't provide a BufferDataUsage then the BufferDataUsage passed in the beggining to the create() function should be used.

Okay, pretty easy.
I just added these 2 member variables to RenderObject:
    BufferDataUsage m_vertexDataUsage = BufferDataUsage::None;
    BufferDataUsage m_indexDataUsage = BufferDataUsage::None;
For their default values I had to add a new value to the BufferDataUsage enum - None.
Then in the create() functions if there is a vertexDataUsage parameter, then the m_vertexDataUsage member is set to that parameter.
That way we remember with what data usage was the render object created. Same for index data.
Then I added an overload of setVertexData() and an overload of setIndexData() that doesn't take in a data usage parameter.
In these 2 overloads we'll just use the 2 new members as data usage, if they are not none
(if they are none it means that the render object was not created with a data usage).
If they are none, we'll use a default data usage.
That's it.

Then I removed the data usage parameter from all calls of setVertexData() and setIndexData() where it was not needed.
All demos work as before.
Done.
}

{
Next feature is a big one:
    Textures.

Let's create a new demo
    Demo04
It will be very similar to Demo00 so let's begin with a copy of Demo00.
The only difference will be that we'll use a texture for the rectangle instead of a gradient color.

First of all, the way we'll read in image files to actual data in C++ is using this library
    stb
More specifically
    stb_image
It's a single-file public domain library. You can get it from here
    https://github.com/nothings/stb
We just need this single header file:
    stb_image.h
and we'll put it under
    dep/stb/
In order for it to work we also need a source file containing these 2 lines
    #define STB_IMAGE_IMPLEMENTATION
    #include <stb/stb_image.h>
Let's create a source file
    stb.cpp
under
    src/PekanCore/Utils
and paste those 2 lines there.
That's it, now we should be able to include <stb_image.h> and use functions from it.

As already said, we will use stb to load image files,
so let's create these 2 new files
    FileUtils.h
    FileUtils.cpp
under
    src/PekanCore/Utils
and write a single function
    const unsigned char* readImageFile(const char* filepath, int& width, int& height, int& numChannels);
that reads in an image from file and returns raw pixel data.
This function is basically a wrapper over
    stbi_load()
so that users of PekanCore can call this Pekan function instead of directly using stb.
It also flips the pixels by first calling
    stbi_set_flip_vertically_on_load(true);

It would be good to also have an Image class inside of PekanRenderer,
that keeps track of the pixel data, width, height, numChannels and other properties that an image might have.
So create a new class Image in 2 files
    Image.h
    Image.cpp
in PekanRenderer. We will need these members:
    const unsigned char* m_data = nullptr;
    int m_width = -1;
    int m_height = -1;
    int m_numChannels = -1;
and getters for each one of them. Then we'll also need a load function
    bool load(const char* filepath);
and for convenience we might also have a constructor for directly loading the image upon creation of an Image instance
    Image(const char* filepath) { load(filepath); }

Added a test image of the Teenage Mutant Ninja Turtles series from 2003 :D
    src/Demo04/resources/tmnt.png
Tried to load it using the Image class.
Seems to work.

Okay, we have images handled.
Now the hard part - implementing the actual textures.
We will create a new class
    Texture
in files
    Texture.h
    Texture.cpp
in
    PekanRenderer
that will be a basic wrapper over an OpenGL texture object.
It will, of course, derive from RenderComponent.
The main thing happening in this Texture class will be assigning an image to the texture object
    void setImage(const Image& image);
We will also have a create() overload that directly does this upon creation of the texture object
    void create(const Image& image);
like that:
    void Texture::create(const Image& image)
    {
        RenderComponent::create(false);
        setImage(image);
    }
Remember that we also have the default create() function coming from RenderComponent that just creates an empty texture object.
Now let's look at the implementation of setImage().
The main thing happening is that we assign the given image to the texture object with this OpenGL function
    glTexImage2D(...)
It requires a bunch of parameters. Some of them are easy to figure out, like the width and height of the image,
we already have those inside of the Image object itself. But some of them are harder to figure out, like the format and internal format.
They depend on the number of channels of the image, and we cannot directly pass the number of channels,
we need to pass GL_RGBA as the format if the number of channels is 4 for example, or we need to pass GL_RGB as the format if the number of channels is 3, etc.
For that we'll add a new static function inside of class Texture
    static void getFormat(const Image& image, unsigned& format, unsigned& internalFormat);
that determines the format and internal format that a texture must have to support the image.
Specifically we need the OpenGL enum values for the format and internal format, and that's exactly what the function fills in to the output parameters
    unsigned& format, unsigned& internalFormat
So that was all about the glTexImage2D() call.
Before it we need to configure a few other things about the texture.
We need to configure the minify and magnify function, and how the texture will wrap.
For now I will configure them with some hardcoded OpenGL values that seem reasonable as defaults.
Later we might need to provide user with control over these values, to allow them to configure them as they wish.
One final thing worth mentioning is the functions
    void bind()
    void unbind()
Textures are special in this regard, we don't just bind a texture,
we bind a texture to a specific texture slot (texture unit).
So we'll add these 2 overloads
    void bind(unsigned slot) const;
    void unbind(unsigned slot) const;
If I could I would remove the parameterless versions of bind() and unbind() because they don't make much sense,
but they are required from the base class RenderComponent. Inside of it bind() and unbind() are pure virtual so we need to implement them.
We will implement them to bind/unbind the texture to the currently active texture slot,
and we'll add a new function in Texture
    static void activateSlot(unsigned slot);
that activates a specific slot.
So now if users of class Texture want to bind a texture object to a specific slot, say 8, they can either do
    texObj.bind(8);
or
    Texture::activateSlot(8);
    texObj.bind();
First one is safer. Would recommend using it.
That's it. That's the Texture class.

Now let's see how we'll use the Texture class.
If we have a Texture object with a loaded image and we bind it to let's say slot 8 how can we actually use that texture?
Well, we need to use texture slot 8 inside of our fragment shader.
The usual workflow is that we have a uniform
    uniform sampler2D tex0;
inside of the fragment shader.
This uniform is supposed to be set from the CPU side to be equal to the desired slot, in this case 8,
so we would do this
    m_shader.setUniform1i("tex0", 8);
Then inside the shader we can use this tex0 together with the texture() function and a texture coordinate like that:
    texture(tex0, vTexCoord);
where vTexCoord is a vec2 containing the coordinates of the point of the texture that we need to sample.

To make all of this more convenient we'll integrate the Texture class inside of class RenderObject.
Let's add a member
    Texture m_texture;
This member will be created and destroyed the same way as other members in the create() and destroy() functions.
To actually set an image to the texture we'll add this new function
    void setTextureImage
    (
        const Image& image,
        const char* uniformName,
        unsigned slot
    );
that assigns the image to the texture like that
    m_texture.setImage(image);
but what it also does is set the uniform inside of the shader to be equal to the slot where the texture will be bound
    m_shader.setUniform1i(uniformName, slot);
How do we make sure that this same slot is the slot where the texture will be bound?
Well, we'll add this new member as well, to RenderObject
    unsigned m_textureSlot = 0xffffffff;
where we'll remember the slot where the texture is supposed to be bound.
So inside of setTextureImage() we will also set
    m_textureSlot = slot;
Then when binding the whole RenderObject we will bind the texture to this same slot, like that
    if (m_textureSlot != 0xffffffff)
    {
        m_texture.bind(m_textureSlot);
    }
This, of course, happens only if an image has been assigned and a slot exists.
Otherwise the texture is empty and won't be used.

So now how do we use a RenderObject with a texture?
We need one final thing, and that's to specify UV coordinates (texture coordinates) of each vertex.
So we'll change the color vertex attribute to be a "texCoord" vertexAttribute.
That's a vertex attribute of type vec2 and it specifies what texture coordinates each vertex should have.
    (The last 2 numbers of each row are the UV coordinates)
    const float vertices[] =
    {
        0.8f,  0.8f, 1.0f, 1.0f, // top right
        0.8f, -0.8f, 1.0f, 0.0f, // bottom right
        -0.8f, -0.8f, 0.0f, 0.0f, // bottom left
        -0.8f,  0.8f, 0.0f, 1.0f  // top left
    };
    ...
    { { ShaderDataType::Float2, "position" }, { ShaderDataType::Float2, "texCoord" } },
Then these texture coordinates will be interpolated for every pixel in between the vertices, so for every pixel inside a triangle,
and we'll receive this varying variable
    in vec2 vTexCoord;
inside of the fragment shader. It will contain the texture coordinates of the current pixel, telling it to which point of the texture it corresponds.
Then we can sample the texture like that
    FragColor = texture(tex0, vTexCoord);
and we have our fragment color.

That's it.
Testing all of that with the TMNT image.
Seems to work. Textures are ready.
Done.
}

----------
17.05.2025
----------

{
Next feature is also a big one
    Wrappers for 2D shapes

I'll do them one by one, starting from triangle.

Before starting let's restructure the files in PekanRenderer a bit, because currently they are all in one place, there are no subdirectories.
Let's create a new directory inside PekanRenderer
    RenderComponents
and inside of it put all the files containing classes that derive from RenderComponent, together with the RenderComponent class itself.
Done.

Let's start with the triangle wrapper now.
Create 2 new files
    TriangleShape.h
    TriangleShape.cpp
under a new directory
    2D/Shapes

Later we might want to have a base class Shape and make TriangleShape derive from it,
but for now let's just implement class TriangleShape basing it on RenderObject.
The way we'll base it on RenderObject will NOT be through inheritance, instead we'll just have a member variable
    RenderObject m_renderObject;

For creating a TriangleShape we will require the 3 vertices obviously.
It would also be good to require a bool parameter called "dynamic" that tells us if we should create the vertex buffer
with dynamic data usage or static data usage.
So the create() function of TriangleShape will look like
    void create
    (
        glm::vec2 vertexA, glm::vec2 vertexB, glm::vec2 vertexC,
        bool dynamic = true
    );
The implementation is straightforward. We just create an array from the 3 vertex positions,
and we call m_renderObject.create() with it, passing the needed parameters.

What about the shader? What shader are we going to pass?
We might give freedom for user to pass their own shader, but for now we'll not do that.
Instead we will have an internal shader.
More specifically, we'll have a 2D vertex shader that takes in 2D vertex positions and does nothing
    src\PekanRenderer\assets\shaders\2D_vertex_shader.glsl
and a "solid color" fragment shader that colors each pixel with a fixed color which is provded via a uniform called "uColor"
    assets\shaders\SolidColor_fragment_shader.glsl
These 2 shaders are not specific to TriangleShape, we will use them for all 2D shapes
when we want to NOT do anything to the vertices on GPU, and to color each pixel a solid color.

So far we had loaded our shaders only in demo applications and the filepaths to those shaders were relative to the demo's working directory.
We cannot do that now. The filepaths of these 2 new shaders are inside of Pekan's folder structure and are relative to Pekan's root,
which is obviously independent from demo's working directory. Demos can be run from anywhere.
What can we do about it?
We can create a macro through CMake that points to Pekan's root or in this case to PekanRenderer's root.
It's done like that:
    target_compile_definitions(PekanRenderer PRIVATE PEKAN_RENDERER_ROOT_DIR="${CMAKE_CURRENT_SOURCE_DIR}")
Now at the moment of generating projects, this macro
    PEKAN_RENDERER_ROOT_DIR
will be set to the specific path to the PekanRenderer directory, in my case
    C:\dev\Pekan\src\PekanRenderer
We can use that macro to get filepaths to the 2 shaders like that:
    #define VERTEX_SHADER_FILEPATH PEKAN_RENDERER_ROOT_DIR "/assets/shaders/TriangleShape_vertex_shader.glsl"
    #define FRAGMENT_SHADER_FILEPATH PEKAN_RENDERER_ROOT_DIR "/assets/shaders/TriangleShape_fragment_shader.glsl"
Now we can easily pass the contents of the 2 shaders to m_renderObject.create() like that:
    Utils::readFileToString(VERTEX_SHADER_FILEPATH).c_str(),
    Utils::readFileToString(FRAGMENT_SHADER_FILEPATH).c_str()

What else?
We need a standard destroy() function,
and we need a render() function that just binds the render object and issues a draw call of 3 vertices
    m_renderObject.bind();
    PekanRenderer::draw(3);

Finally, we need to set the color uniform "uColor" inside render object's shader
    m_renderObject.getShader().setUniform4fv("uColor", { 1.0f, 0.0f, 0.0f, 1.0f });
For now setting a solid red color. Next commit we'll have to provide users of TriangleShape with control over the color.

Let's test it in Demo04.
Just draw a small triangle somewhere on screen, together with the textured rectangle.
It works!

Now let's allow user to set a color, instead of just hardcoding red.
Add these functions to TriangleShape
    inline glm::vec4 getColor() const { return m_color; }
    void setColor(glm::vec4 color);
and this member
    glm::vec4 m_color;
and then in setColor() we'll obviously set m_color to the given color,
but we'll also set the "uColor" uniform inside the shader, but only if the shader is valid (created)
    m_color = color;
    if (m_renderObject.getShader().isValid())
    {
        m_renderObject.getShader().setUniform4fv("uColor", color);
    }
That's it.
Test it in Demo04 by animating the color.
It works!

Now let's allow the triangle to be moved.
We can easily add a setVertices() function that just sets 3 entirely new vertices.
This is fine and we'll do it, but let's also have a function setPosition() that sets the position of triangle's origin.
Vertex positions will be considered relative to that origin.
We can also have a move() function that adds a deltaX and deltaY to triangle's origin.
So I added these 2 functions
    void setPosition(glm::vec2 position);
    void move(glm::vec2 deltaPosition);
and these members
    glm::vec2 m_position = glm::vec2(0.0f, 0.0f);
    glm::vec2 m_vertices[3] = { glm::vec2(0.0f, 0.0f), glm::vec2(0.0f, 0.0f), glm::vec2(0.0f, 0.0f) };
and some getters for them.
It works!

That's the TriangleShape basically.
Done.
}

----------
19.05.2025
----------

{
Before moving on to the next shape class, let's create a base Shape class
by extracting all the shape-common logic from TriangleShape.

Did it.
Looks pretty clean now!
Class Shape contains the position, color and render object and it defines all common actions.
The responsibility of class TriangleShape is only to provide the logic around the 3 vertices specifically.

I also added functionality for setting each individual vertex after having created the TriangleShape.
Tested it in Demo04. Works pretty well.

Code looks solid!
Done.
}

----------
20.05.2025
----------

{
Now that we have a good base class Shape let's create the next 2D shape wrapper
    RectangleShape

Did it.
Mostly straightforward. One interesting thing that's worth mentioning
is that we had made class Shape support only RenderObjects without indices and rendering without indices.
So I had to add support for indices in Shape.
Added this member
    bool m_usingIndices = false;
which is a flag telling us if the shape is using indices.
Also added this overload of createRenderObject()
    void createRenderObject(const void* vertexData, const void* indexData, bool dynamic);
that takes in indices.
So now if the Shape is created with this overload then m_usingIndices will be true.
We care about this flag when we want to render the Shape, so inside of the render() function we can do that
    if (m_usingIndices)
    {
        PekanRenderer::drawIndexed((getNumberOfVertices() - 2) * 3);
    }
    else
    {
        PekanRenderer::draw(getNumberOfVertices());
    }
I also added this function for convenience
    inline int getIndexDataSize() const { return (getNumberOfVertices() - 2) * 3 * sizeof(unsigned); }
for calculating the (expected) size of the index data.

What else?
The RectangleShape is a bit different from TriangleShape in that it doesn't give control over the 4 different vertices.
Instead a rectangle has a width and height and this is what users can provide.
The position coming from Shape is used as the position of the bottom left corner of the rectangle.
So if you position a rectangle on (X, Y) that's going to be the position of its bottom left corner.
So wherever you place a rectangle you are placing its bottom left corner.
Everything else is pretty standard.

Tested in Demo04. Works pretty well.
Done.
}

{
Next 2D shape wrapper
    CircleShape

Circles are a bit more interesting, but also a bit simpler.
They only have a radius so we'll need this member
    float m_radius = 0.0f;
plus getters and setters for it
    void setRadius(float radius);
    inline float getRadius() const { return m_radius; }
The create() function will take in a radius
    void create
    (
        float radius,
        bool dynamic = true
    );
How is it going to create the circle? What are the vertices of a circle?
Well, the standard way to render a circle is approximating it with a regular N-gon.
In other words dividing the circle's circumference into N equal parts,
and forming N triangles with the center, giving you a triangle fan.
For that to work we will have to issue a draw call with
    DrawMode::TriangleFan
instead of the default
    DrawMode::Triangles
Since the code that makes the draw call is inside of the base class Shape - inside of the render() function,
we need some way for derived classes to specify their draw mode.
So let's add this virtual function in class Shape
    virtual DrawMode getDrawMode() const { return DrawMode::Triangles; }
that returns the draw mode that needs to be used for rendering.
By default shapes will be rendered with DrawMode::Triangles
unless they override this function to return something else.
In the case of CircleShape we want it to be that:
    virtual DrawMode getDrawMode() const { return DrawMode::TriangleFan; }

Alright, now how do we generate the vertices of a circle?
Well, it depends on the number of segments, and it's good to make this number be configurable.
Add this member to class CircleShape
    int m_segmentsCount = 0;
together with a getter and a setter
    void setSegmentsCount(int segmentsCount);
    inline int getSegmentsCount() const { return m_segmentsCount; }
Then in the create() function we'll set this member m_segmentsCount equal to some default number of segments
    static const int DEFAULT_SEGMENTS_COUNT = 42;
    ...
    m_segmentsCount = DEFAULT_SEGMENTS_COUNT;
Now, generating the vertices obviously depends on the number of segments,
so we will have a different number of vertices based on the current number of segments.
This means we can't no longer have a static array of a constant number of elements, we need a vector
    std::vector<glm::vec2> m_vertices;
Then let's create a function
    void generateVertices();
that generates (or regenerates) circle's vertices based on the current number of segments.
The first vertex will be the center
    m_vertices.clear();
    m_vertices.push_back({ 0.0f, 0.0f });
Then we need m_segmentsCount + 1 more vertices placed evenly around the circle.
Why m_segmentsCount + 1 ?
Well, because the first and last vertex need to be at the same position to "close the loop".
The way the GPU renders a triangle fan is it draws a triangle from vertices 0, 1, 2, then 0, 2, 3,
then 0, 3, 4, ... , 0, N-3, N-2, then 0, N-2, N-1, where N is the number of vertices in the vertex buffer.
In our case vertex 0 is the center, then we want vertex 1 to be at
    cos(0), sin(0)
and vertex 2 to be at
    cos(2 * pi / m_segmentsCount)
    sin(2 * pi / m_segmentsCount)
and vertex 3 to be at
    cos(2 * 2 * pi / m_segmentsCount)
    sin(2 * 2 * pi / m_segmentsCount)
and vertex 4 to be at
    cos(3 * 2 * pi / m_segmentsCount)
    sin(3 * 2 * pi / m_segmentsCount)
etc.
The last vertex needs to be the same as the first one
    cos(0) = cos(m_segmentsCount * 2 * pi / m_segmentsCount)
    sin(0) = sin(m_segmentsCount * 2 * pi / m_segmentsCount)
so that the final triangle that connects the last point to the first point is also drawn.
(Otherwise the circle will be drawn as a pie with a missing piece)
So that's how we generate the vertices around the circle
    for (int i = 0; i <= m_segmentsCount; i++)
    {
        const float angle = i * 2.0f * PI / m_segmentsCount;
        const float x = m_radius * cos(angle);
        const float y = m_radius * sin(angle);
        m_vertices.push_back({ x, y });
    }

This function
    generateVertices()
will have to be called also when setting a new number of vertices
    void CircleShape::setSegmentsCount(int segmentsCount)
    {
        m_segmentsCount = segmentsCount;
        generateVertices();
        updateRenderObject();
    }

That's about it.
Tested it in Demo04 with 2 circles,
one of them has number of segments changing with time,
and the other has position, radius and color changing with time.
Seems to work pretty well.
Done.
}

----------
22.05.2025
----------

{
A little optimization can be done for CircleShape.
Currently it keeps its vertices in a vector like that
    std::vector<glm::vec2> m_vertices;
which is okay, but it uses dynamic memory.
Not perfect for a large number of circles.
Imagine a game where 1 million circles have to be rendered,
and all of them have the same fixed number of segments that is not going to change.
Then it makes more sense to store those circles in static memory.
Of course this would remove the ability to change number of segments dynamically,
which is not great - there might be scenarios where you want to change the number of segment dynamically,
for example depending on the size of the circle, when moving the camera.
In conclusion, we need both options.
So I will add another class
    class CircleShapeStatic
which will have a static fixed number of segments that cannot be change during the lifetime of an object.
The only compile-time way to specify number of segments is through a template parameter,
so CircleShapeStatic will need to be a template class
    template <unsigned NSegments = 42>
    class CircleShapeStatic : public Shape
and then the vertices can be kept like that:
    glm::vec2 m_vertices[NSegments + 2];
This change from vector to a static array leads to changes to the implementation of some functions,
but it's pretty straightforward.
One annoying aspect of using a template class is that we cannot have a .cpp file with function definitions anymore.
We need to have the definitions inside of the header, or a cleaner solution is to have another header
    CircleShapeStatic_impl.h
containing the definitions of the functions, and then we can just include this new header at the end of the old one
    #include "CircleShapeStatic_impl.h"

That's about it.
Testing it in Demo04. Changed the second circle to be a CircleShapeStatic
    Pekan::Renderer::CircleShapeStatic<> m_circleStatic;
Works same as before.
Done.
}

----------
23.05.2025
----------

{
Let's take a look at this TO-DO item
    Think about how many and which ones of the create() overloads in RenderObject we need.

Yeah I guess it makes sense to have only 2 overloads of create().
One with vertex data and one without.
The index data should be provided through a set method, same as a texture.
So basically remove the first create() overload and change the client code wherever this overload was used
to instead use the next overload and then call setIndexData(...).
Done.
}

{
Look at next TO-DO item
    Check what happens if you call setTextureImage() more than once on a single RenderObject

It doesn't work of course.
I mean it will work with only your last call. Previous calls will be overwritten.
That's because we currently support only one texture per RenderObject.

Let's implement support for multiple textures in RenderObject.
Basically, instead of a single texture we need a vector of texture pointers
    std::vector<TexturePtr> m_textures;
At each index i, the texture will correspond to slot i.
So if at index i we have a non-null pointer this means that a texture exists, has been created by user,
and should be bound to slot i.
We will modify the setTextureImage() function to grow the m_textures vector whenever the requested slot exceeds the current size of the vector.
    if (slot >= m_textures.size())
    {
        m_textures.resize(slot + 1);
    }
Then at the requested slot we will just create a new texture from the given image
    m_textures[slot] = std::make_shared<Texture>();
    m_textures[slot]->create(image);
That's the essential part.

When growing the vector it'd be good to check if the requested slot is not too big.
There is a GPU-imposed limit on the number of available texture slots.
Let's add a function in PekanRenderer that retrieves this GPU limit
    static int getMaxTextureSlots();
and then in setTextureImage() before proceeding to doing anything, check if requested slot is less than the limit
    if (int(slot) >= PekanRenderer::getMaxTextureSlots())
    {
        ... log error ...
        return;
    }

What's left is to act on the vector of textures instead of a single texture in all other places,
for example in bind() we need to do this
    for (unsigned i = 0; i < m_textures.size(); i++)
    {
        if (m_textures[i] != nullptr)
        {
            PK_ASSERT_QUICK(m_textures[i]->isValid());
            m_textures[i]->bind(i);
        }
    }
Go through all textures, and bind the ones that are not null, making sure that they are valid with a quick assert.

That's pretty much it.
Let's test it in Demo04 by having 2 images, one of the TMNT and one of the powerpuff girls
    static const char* IMAGE0_FILEPATH = "resources/tmnt.png";
    static const char* IMAGE1_FILEPATH = "resources/powerpuff.png";
and then blending between the two of them in the fragment shader like that
    vec4 tex1 = texture(uTex0, vTexCoord);
    vec4 tex2 = texture(uTex1, vTexCoord);
    FragColor = mix(tex1, tex2, uMixFactor);
based on a mix factor uMixFactor which is a uniform that we will set in the scene's update() function to oscillate between the two images
    texRectShader.setUniform1f("uMixFactor", osc(t / 5.0f));

----------
24.05.2025
----------

Running Demo04 it looks quite busy, you can't clearly see the 2 textures being blended together,
so let's add a checkbox that hides/shows all shapes
    ImGui::Text("Enable Shapes");
    ImGui::Checkbox("##", &m_enabledShapes);
Then in scene's render() function we will render the shapes only if this checkbox is enabled.

Side note: There is a small bug when you add an image under the same texture slot multiple times
by calling setTextureImage() multiple times. It has to do with the way we keep TexturePtr's in a vector.
When a Texture object gets automatically destroyed by the vector we want the destroy() function to be called.
However, this should happen only if the Texture object is valid.
So basically we need this destructor of class Texture
    Texture::~Texture()
    {
        if (isValid())
        {
            destroy();
        }
    }
To be honest it's a good idea to do the same for all other RenderComponents.
Annoyingly we cannot do this in RenderComponent's destructor because the destroy() function
calls unbind() which is a pure virtual function in RenderComponent and it's defined in derived classes,
but RenderComponent's destructor is called after a derived class' constructor so at this point the unbind() function is destroyed and cannot be called.
So let's just add this destructor to each derived class of RenderComponent.
Done. All demos work the same way, and don't crash on closing.
}

{
I have a small fun side quest in mind.
Let's try to do some shader effects with the 2 textures in Demo04.

For that it'd be cool to be able to dynamically switch shaders at runtime through GUI.
Not sure if this will require any changes in engine code. It'll be interesting to see if it can be done with current engine.

Nope, it cannot be done with current engine.
We need a function in RenderObject that sets new shader source
    void setShaderSource(const char* vertexShaderSource, const char* fragmentShaderSource);
The essential thing it should do is
    m_shader.setSource(vertexShaderSource, fragmentShaderSource);
It also needs to clear the textures vector
    m_textures.clear();
because current textures are bound to the current shader (to the current shader's source code).
They correspond to slots and uniforms in the current shader's source code.
Changing the shader's source code invalidates the textures.
Importantly, this means that client code should remember to set their textures again after changing shader's source code,
but this is pretty normal and expected because normally when you change shader's source code you don't want to keep the exact same textures
bound to the exact same slots and uniforms. You usually completely change the textures.
Nevertheless, I created a TO-DO item for this, to think about it more carefully later, because I'm not fully sure now.

There is a problem now. The setSource() function of class Shader doesn't work when calling it after having created the shader already.
So far we have used it only for creating the shader at the beginning of its lifetime with create(),
but we have never tried using it to reset source.
It doesn't work. Why?
Because the previous vertex shader and fragment shader are not detached and deleted,
and when you add new vertex and fragment shader they are added to the old ones, they don't override the old ones,
so now we have 2 main functions in the merged fragment shaders, and 2 main functions in the merged vertex shaders,
which results in a linking error.
So, we need to first detach and delete the old ones.
Let's create a function for that
    void detachAndDeleteShaders();
Details not important.
So if we already have shaders attached at setSource() then we'll first call detachAndDeleteShaders().
Now it works.

One shader effect that I have in mind is to have 2 points in the plane, in the fragment shader.
They may be hardcoded at (0.25, 0.25) and (0.75, 0.75) (in texture space) for now, later we might make them move.
   vec2 p0 = vec2(0.25, 0.25);
   vec2 p1 = vec2(0.75, 0.75);
Then each pixel will be sampled from a mix of tex0 and tex1 based on its distance to the 2 points.
More precisely the closer a pixel is to p0 the more it will be sampled from tex0,
and the closer it is to p1 the more it will be sampled from tex1.
However this is not exactly the formula that we will use. It will NOT be based on distance exactly.
I want points that are completely from one side of p0 to be sampled completely from tex0.
For example the pixel (0.1, 0.2) should be completely sampled from tex0.
More precisely, we'll look at the direction from p0 to p1
    vec2 p0p1 = vec2(p1.x - p0.x, p1.y - p0.y);
and for any pixel (x, y) we'll consider the distance from p0 to (x, y) ALONG the direction p0->p1.
    float p0dist = dot(p0v, normalize(p0p1));
where p0v is just the vector from p0 to (x, y)
    vec2 p0v = vec2(vTexCoord.x - p0.x, vTexCoord.y - p0.y);
Then dividing this distance by the distance from p0 to p1
    float blend = p0dist / distance(p0, p1);
will put it in such range that pixels in between p0 and p1 will get assigned a number from 0 to 1,
pixels to "the left" (in the slanted direction described above) of p0 will be < 0,
and pixels to "the right" of p1 will be > 1.
We will, of course, clamp this value to the range [0, 1] so that it can be used for a mix between the 2 textures.
    blend = clamp(blend, 0.0, 1.0);
    ...
    FragColor = mix(tex0, tex1, blend);
That's it. That's the shader effect. Calling it "Two Points Blend".

----------
25.05.2025
----------

Now let's make the points move.

Also, we can make the points themselves visible.
There are 2 ways to do that. First way, we can draw circles on top of the points.
Second way is a more shader-based approach, and we'll go with it,
We will pass only the time as a uniform to the shader,
and let the shader itself generate and draw the points.
How will the shader draw the points?
Well, it just needs to color the pixels close enough to the points in a certain color, say black.
Done. Points are now visible.

Let's make them move in a circle and always being on two opposite sides of the circle.
Done.

Added 2 more shaders that move the two points in different ways.
That's it. Calling it done here.
Multiple textures work now, and dynamically changing shaders works.
Done.
}

{
Okay, I did some TO-DO items, now it's time to continue with the next 2D shape wrapper - a line

There are 2 distinct ways to render a line.

First way, is to create the line from 2 slim triangles.
A good thing about this approach is that it supports different thicknesses for the lines.
A bad thing is that it may be a bit slower.

Second way, is to just draw a line primitive with OpenGL
    glDrawArrays(GL_LINES, 0, 2);
This is a faster way, but thickness can't be provided.

I will do both - class LineShape will be a line made up of 2 triangles,
and class Line will be just a line primitive.

Let's start with class Line.
Should it derive from class Shape? No, not really,
because we don't need a position for the line, and because we'll do the draw call differently.
Instead, we'll have a member
    RenderObject m_renderObject;
and we'll managed it in a way similar to how class Shape manages its m_renderObject.
We'll use the same vertex and fragment shaders.
It's pretty straightforward.
The actual draw call will look like
    PekanRenderer::draw(2, DrawMode::Lines);

Let's create a new demo
    Demo05
for testing lines.
Later I'll do a Mandelbrot Timestable there,
it's made up entirely from lines and it looks cool.

Class Line works perfectly fine.
However, I see that the line is jagged, doesn't look great.
That's because we don't do anti-aliasing currently, so let's do it now.
It's quite simple in OpenGL to enable MSAA (Multisample Anti-Aliasing)
It's just this OpenGL call
    GLCall(glEnable(GL_MULTISAMPLE));
and the GLFW window should be configured to use more than 1 sample per pixel, like that
    glfwWindowHint(GLFW_SAMPLES, 8);
Instead of puttin 8 there, let's add a new property inside of WindowProperties
    int numberOfSamples = 1;
and then applications will be able to set this property before creating their window.
What about the OpenGL call for enabling MSAA? Let's wrap that in a static function in PekanRenderer
    static void enableMultisampleAntiAliasing();
That's it, now in Demo05_Scene::init() we can enable MSAA like that
    PekanRenderer::enableMultisampleAntiAliasing();
and in Demo05_Application::_init() we can set the number of samples per pixel like that
    windowProperties.numberOfSamples = 16;
It works! Line looks pretty good now.

----------
27.05.2025
----------

Now let's implement the LineShape class as well,
creating the line from 2 thin triangles.
It will be very similar to other shape classes like RectangleShape.
The essential difference is how we generate vertices from point A, point B and a thickness.
Let's create a private member function for that
    void generateVertices();
We basically need to turn the 2 points A and B into 4 points based on the thickness T.
So A will turn into 2 points and B will turn into 2 points.
The way to turn an original point into 2 points is to add a vector of length T / 2 in the 2 directions perpendicular to AB.
    const glm::vec2 dir = glm::normalize(m_pointB - m_pointA);
    const glm::vec2 normalOffset
    (
        -(m_thickness / 2.0f) * dir.y,
        (m_thickness / 2.0f) * dir.x
    );
The normal offset vector is just that - a vector in one of the perpendicular directions with length T / 2.
The vector in the other perpendicular direction will be just the negative of that vector.
So we can get the 4 vertices like that
    m_vertices[0] = m_pointA + normalOffset;
    m_vertices[1] = m_pointA - normalOffset;
    m_vertices[2] = m_pointB - normalOffset;
    m_vertices[3] = m_pointB + normalOffset;
That's pretty much it. Everything else is very standard.

Let's test it in Demo05.
We'll make the thickness change with time just to see if it's fully correct.
Also let's change colors and opacity with time.
It works!

Now let's test it more extensively in Demo05 by implementing a Mandelbrot Times Table,
or better yet 1 MTT using class Line and 1 MTT using class LineShape.
Done. Looks good.

Let's quickly add a slider in GUI that controls the thickness of LineShapes.
Done.

This concludes the subject of lines. We have class LineShape and class Line.
}

----------
28.05.2025
----------

{
We have one last 2D shape to implement - polygon.

A polygon is any shape made up of some vertices connected in a loop.
Technically all shapes so far have been polygons, but now we need a general polygon.
User should be able to provide a list of vertices and form a polygon with them.

So let's create a new class
    class PolygonShape : public Shape
The logic is mostly straightforward. This time we will not have any additional data except the vertices
    std::vector<glm::vec2> m_vertices;
In the create() function we'll take in a list of vertices
    void create
    (
        const std::vector<glm::vec2>& vertices,
        bool dynamic = true
    );
and literally just set m_vertices equal to that given list of vertices and then create the render object
    m_vertices = vertices;
    Shape::createRenderObject(m_vertices.data(), dynamic);
The way we'll render a polygon will be using a TriangleFan primitive, same as the circle.
    virtual DrawMode getDrawMode() const { return DrawMode::TriangleFan; }
However, this will work only for convex polygons.
For now we will allow only convex polygons.
Later we must implement a triangulation algorithm that splits the polygon into triangles in a good way,
so that we can support non-convex polygons as well.
For now we will check that the given vertices form a convex polygon, and if not we'll log an error message.
Let's create a function that checks if a polygon is convex
    bool isConvex() const;
The idea here is simple - we just loop over all triplets of 3 consecutive vertices A, B, C,
and check if they form an angle that turns left or right.
This is equivalent to seeing if the cross product of AB and BC is positive or negative.
In order for a polygon to be convex we want all triplets A,B,C to turn the same direction,
either all left or all right, which is equivalent to all of their cross products being either positive or negative.
That's it. That's how isConvex() works.
We'll use this function to check that the vertices given in create() form a convex polygon and log an error message otherwise.
    if (!isConvex())
    {
        PK_LOG_ERROR("Class PolygonShape supports only convex polygons, but you are creating a non-convex one. "
            "It might not be rendered correctly", "Pekan");
    }
We must do a similar check in setVertices() and setVertex().
This check might be a bit expensive for performance though, and it's only really useful in development,
so let's do all this convex stuff only in Debug builds.
Also, it's temporary. We'll implement a triangulation algorithm in v0.3.

Side note, it'd be usedful for PolygonShape to have a function that returns the number of vertices.
We already have this function
    int getNumberOfVertices() const override { return m_vertices.size(); }
Only problem is that it's private (or protected). It comes from class Shape.
Class Shape needs this function to be implemented by derived classes,
but there is no reason it has to be protected. We just didn't need it to be public until now.
So let's just make it public
    public:
        ...
        int getNumberOfVertices() const override { return m_vertices.size(); }
It will have to be public in class Shape and in all derived classes.
Done.

Side note, enabled MSAA in Demo02 and Demo01, they look a bit better now.

That's basically it.
Now let's test PolygonShape in Demo04.
It works! Looks pretty good.

With that we are officially DONE with 2D shapes!
}

{
Let's look at this TO-DO item
    Research glBufferSubData(). Decide if we need it in Pekan.

glBufferSubData() updates a portion of an already allocated buffer object without reallocating or redefining the entire buffer.
It's definitely useful to have that in class VertexBuffer and IndexBuffer for cases when user wants to set data of just a part of a buffer.

So let's just add this new function
    void setSubData(const void* data, long long offset, long long size);
to both VertexBuffer and IndexBuffer. The function signature looks exactly the same for both of them.
The implementation is almost the same, only difference is the target buffer.
In the case of VertexBuffer it's this
    void VertexBuffer::setSubData(const void* data, long long offset, long long size)
    {
        bind();
        GLCall(glBufferSubData(GL_ARRAY_BUFFER, offset, size, data));
    }
and for IndexBuffer it's the same but with GL_ELEMENT_ARRAY_BUFFER instead of GL_ARRAY_BUFFER.
Pretty easy.

Question is how do we test it?
That will be a bit tricky. I don't want to create a whole new demo just for that.
So let's test it quickly in an old demo in a sneaky way.

We can test VertexBuffer's setSubData() in Demo01 by making the third (why not?) square move slightly every frame,
if there is a third square at all. Since all squares are in the same buffer we can use setSubData()
to change only the third one.
In order for that to work we actually need to add 2 wrapper functions in RenderObject as well
    void setVertexSubData(const void* data, long long offset, long long size);
    void setIndexSubData(const void* data, long long offset, long long size);
Okay, easy.

By the way, do we need to check if the given region is within the bounds of the buffer?
Not really. We are not checking anything like that, because raw RenderComponents will not be directly used by users,
they will be used by other classes in PekanRenderer like the 2D shapes.
We must choose the levels on which to do checks. We cannot do all checks on all levels, it'd be bad for performance.

So let's continue now with making the third square in Demo01 move slightly.
Done. It works.
Let's also add a checkbox in GUI that enables/disables this movement of the third square.

I guess for IndexBuffer's setSubData() function we can do something similar
in Demo02 by adding a checkbox in GUI that reverses the order of indices of one of the sides of the cube.
When enabled, that side of the cube should be invisible.
Let's do it with the fourth side.
Done. It works.
}

----------
29.05.2025
----------

{
Let's look at this TO-DO item
    Create enums for parameters in class Texture

First, we'll need 2 new enum types in PekanRenderer for minifying and magnifying functions
    TextureMinifyFunction
    TextureMagnifyFunction
Their values will correspond to the values defined in the OpenGL documentation.
We'll have these 2 static functions in PekanRenderer for converting from our enum types to OpenGL enums
    static unsigned getTextureMinifyFunctionOpenGLEnum(TextureMinifyFunction function);
    static unsigned getTextureMagnifyFunctionOpenGLEnum(TextureMagnifyFunction function);
Then in class Texture we'll have 2 new public functions for setting a minify/magnify function
    void setMinifyFunction(TextureMinifyFunction function);
    void setMagnifyFunction(TextureMagnifyFunction function);
and we'll use them in the create() function with default values
    setMinifyFunction(DEFAULT_TEXTURE_MINIFY_FUNCTION);
    setMagnifyFunction(DEFAULT_TEXTURE_MAGNIFY_FUNCTION);
where these default values are
    static const TextureMinifyFunction DEFAULT_TEXTURE_MINIFY_FUNCTION = TextureMinifyFunction::LinearOnLinearMipmap;
    static const TextureMagnifyFunction DEFAULT_TEXTURE_MAGNIFY_FUNCTION = TextureMagnifyFunction::Linear;
That's basically it.
Now if someone wants to change the minify/magnify function they can use the setMinifyFunction() and setMagnifyFunction() functions.
We'll not test that for now, because we don't have an appropriate scene for it, and it's not that important.

----------
30.05.2025
----------

Next enum that we'll need for textures is the wrapping mode.
Add this new enum in PekanRenderer
    enum class TextureWrapMode
    {
        ClampToEdge = 0,
        Repeat = 1,
        MirroredRepeat = 2,
        ClampToBorder = 3
    };
together with a static function in class PekanRenderer that maps this Pekan enum to an OpenGL enum value
    static unsigned getTextureWrapModeOpenGLEnum(TextureWrapMode wrapMode);
Then in class Texture add these 2 new functions
    void setWrapModeX(TextureWrapMode wrapMode);
    void setWrapModeY(TextureWrapMode wrapMode);
for setting a wrap mode for X and Y texture coordinates.
We'll call these 2 functions in the create() function
    setWrapModeX(DEFAULT_WRAP_MODE_X);
    setWrapModeY(DEFAULT_WRAP_MODE_Y);
where these default values are
    static const TextureWrapMode DEFAULT_WRAP_MODE_X = TextureWrapMode::ClampToBorder;
    static const TextureWrapMode DEFAULT_WRAP_MODE_Y = TextureWrapMode::ClampToBorder;
That's it for this enum.

A related thing that we need is to have this inside a function
    GLCall(glTexParameterfv(GL_TEXTURE_2D, GL_TEXTURE_BORDER_COLOR, &DEFAULT_BORDER_COLOR.x));
in class Texture
    void setBorderColor(glm::vec4 color);
That's it.

We will not test any of that, because we don't have an appropriate scene for it, and it's not that important.
Solution compiles and all demos work as before.
Done.
}

----------
04.06.2025
----------

{
Let's look at this TO-DO item
    Check if passing vertices to TriangleShape's create() in reverse order still works (change clockwise-ness of vertices).

First of all, obviously this depends on whether face culling is enabled.
If it's NOT enabled, then all shapes will be visible no matter the orientation of their vertices.
If it's enabled
    PekanRenderer::enableFaceCulling();
then, indeed, shapes will be hidden if their vertices are in clockwise order.
I verified this by reversing the order of triangle's vertices in Demo04
    //m_triangleInitialVertexA = { -0.1f, -0.1f };
    //m_triangleInitialVertexB = { 0.1f, -0.1f };
    //m_triangleInitialVertexC = { 0.1f, 0.1f };
    m_triangleInitialVertexA = { 0.1f, 0.1f };
    m_triangleInitialVertexB = { 0.1f, -0.1f };
    m_triangleInitialVertexC = { -0.1f, -0.1f };

By the way, this problem applies only to class TriangleShape and class PolygonShape,
not to other Shape classes. That's because for other Shape classes we implicitly create the vertices
from given user parameters, and we always create them in CCW order.
Only class TriangleShape and class PolygonShape give control over each vertex,
and hence have the possibility of having CW vertices.

Is this bad?
Answer is not very clear.
If user has enabled face culling, they want their triangles to be invisible from the back.
But do they want their TriangleShapes to be invisible from the back?
Not sure. Probably not. It's a design choice.
It makes more sense to me at this moment that a TriangleShape should always be visible,
and the face culling should apply to FACES of meshes - they are triangles but not TriangleShapes.
So TL;DR - It is bad. We want all shapes to be always visible no matter the order of vertices.

We can definitely do that, but is there a downside to doing it?
Well, a downside would be performance.
Checking if vertices are ordered correctly, and if not, reordering them, is some additional CPU work.
So what do we do?
Should we always check orientation and lose some performance?
Well, we should definitely NOT check orientation if face culling is disabled, I think this is pretty clear.
If face culling is enabled, should we check orientation by default?
Yes, I think so.
We can have a configurable option to disable orientation checking, as an optimization.
Should this option be per-shape? Or should it be global?
Global makes more sense at this point of development.
Should it be a global runtime option?
So that it can be enabled at some point and then disable at another point?
No, probably not needed.
Better for performance if it's a compile time option, so basically a macro that can be set through CMake or as a #define.

So let's actually do it now.
First, introducing this new option in PekanRenderer's CMake file
    option(PEKAN_DISABLE_2D_SHAPES_ORIENTATION_CHECKING "Disable orientation checking for 2D shapes" OFF)
and setting it as a compile definition
    target_compile_definitions(PekanRenderer PRIVATE
        ...
        PEKAN_DISABLE_2D_SHAPES_ORIENTATION_CHECKING=$<IF:$<BOOL:${PEKAN_DISABLE_2D_SHAPES_ORIENTATION_CHECKING}>,1,0>
    )
Then in class TriangleShape we'll introduce this new member variable
    unsigned m_indices[3] = { 0, 1, 2 };
but only if PEKAN_DISABLE_2D_SHAPES_ORIENTATION_CHECKING is 0.
Together with it we'll add this new function
    void updateIndicesOrientation();
that will update the m_indices member to be { 0, 1, 2 } or { 2, 1, 0 } depending on whether
the 3 vertices in m_vertices are currently in CCW or CW order.
Then each time that vertices are changed in TriangleShape we'll need to call this function
to update the indices, making sure that triangle is drawn in CCW order.
That's not all.
If PEKAN_DISABLE_2D_SHAPES_ORIENTATION_CHECKING is 0 we need to pass the indices to Shape::createRenderObject() like that:
    Shape::createRenderObject(m_vertices, m_indices, dynamic);
If PEKAN_DISABLE_2D_SHAPES_ORIENTATION_CHECKING is 1 we'll just not use indices at all
    Shape::createRenderObject(m_vertices, dynamic);
Same kind of logic applies for updating the render object
    updateRenderObject(m_indices);
in the first case, and
    updateRenderObject();
in the second.
By the way, such function updateRenderObject(m_indices) doesn't exist, lol.
So let's create it in class Shape
    void updateRenderObject(const void* indexData);
It just needs to call setVertexData() and setIndexData() on the underlying render object.
    m_renderObject.setVertexData(getVertexData(), getVertexDataSize());
    m_renderObject.setIndexData(indexData, getIndexDataSize(), BufferDataUsage::DynamicDraw);

That's the crux of it.
Now we need to NOT do this whole orientation checking if face culling is NOT enabled.
How do we check if face culling is enabled?
Currently we can't.
We need to implement this in PekanRenderer.
Let's add this static member variable
    static bool s_isEnabledFaceCulling;
together with this static function
    static inline bool isEnabledFaceCulling() { return s_isEnabledFaceCulling; }
The initialization value of the member variable will be false
    bool PekanRenderer::s_isEnabledFaceCulling = false;
because face culling is NOT enabled when you start the engine.
Then inside of PekanRenderer::enableFaceCulling() we'll set it to true
    s_isEnabledFaceCulling = true;
For consistency let's add a function for disabling face culling because currently we can only enable it and leave it enabled forever.
    static void disableFaceCulling();
It needs to disable face culling and, of course, set the member variable to false
    s_isEnabledFaceCulling = false;
Now inside of TriangleShape's updateIndicesOrientation() function we can first check if face culling is enabled,
and if it's not, we can just skip and not do anything
    if (!PekanRenderer::isEnabledFaceCulling())
    {
        return;
    }
to not waste performance reorienting triangle vertices when face culling is off.

That's about it.
Did a few other small changes, but nothing important.

Tested it in Demo04 by adding 2 checkboxes,
one that reverses the order of triangle's vertices from CCW to CW,
and one that enables face culling.
It works as expected in all cases.
Basically the triangle is always visible unless we turn on the option
    PEKAN_DISABLE_2D_SHAPES_ORIENTATION_CHECKING
and enable face culling, and reverse the order of triangle's vertices.
In that case triangle is invisible, which is exactly what we want.

What's left now is to do the same thing with PolygonShape.
However, I started doing it and it's too complicated and invloved.
It introduces some design problems with the base class Shape.
Biggest problem is that in order to do this with PolygonShape we need to use a vector of indices
and its size IS NOT correctly computed by getIndexDataSize() because getIndexDataSize()
works with the assumtion that we're using triangles as the primitive, not triangle fan.
Class CircleShape uses triangle fan, but it doesn't use indices.
Basically using triangle fan + indices is problematic in class Shape.
So, decided NOT to do this in class PolygonShape.
That's not so bad because PolygonShape will be redesigned in the future to use the ear-clipping algorithm
and then it will use triangles as primitive.

For now, we can just check the orientation of the polygon in debug mode,
and log a warning message if it's CW and face culling is enabled,
to let the developer know why their polygon is not showing.
Done.

Added a checkbox in the GUI of Demo04 for reversing the orientation of polygon's vertices.
Works as expected.
Done.
}

----------
06.06.2025
----------

{
Next TO-DO item
    Think about using getVertexData() in createRenderObject() instead of having a vertexData parameter

Seems like a reasonable thing to do at first sight.
There is already a pure virtual function
    virtual const glm::vec2* getVertexData() const = 0;
in class Shape, which is implemented by all derived classes,
and it's used in Shape's updateRenderObject() function (both overloads).
So why not use it in createRenderObject() as well, instead of having a vertexData parameter?
Let's try.

Okay, pretty easy.
It build and works same as before.

Let's do the same with indices now, seems to make sense for the same reason.
This time we don't already have a virtual function getIndexData() so let's create it
    virtual const unsigned* getIndexData() const { return nullptr; }
I will NOT make it pure virtual because some (most) derived classes will not use indices at all,
so they can just skip overriding this function and they should work like that.
Then we'll remove the indexData parameter from the createRenderObject() function.
This leaves us with a single overload, because both overloads would be parameter-less now.
Until now the way we knew if we're using indices or not was based on which overload of createRenderObject() is called.
Obiously we can't do that anymore, because now we have a single overload.
Luckily, we can use the return value of getIndexData() instead. If it's NULL then we'll NOT use indices, otherwise we WILL use indices. That's even easier.
    const unsigned* indexData = getIndexData();
    m_usingIndices = (indexData != nullptr);
    if (m_usingIndices)
    {
        m_renderObject.setIndexData(indexData, getIndexDataSize(), BufferDataUsage::StaticDraw);
    }
What's left now is to implement getIndexData() in all derived classes that use indices.
Done.

It builds and Demo04 works as before, with and without the PEKAN_DISABLE_2D_SHAPES_ORIENTATION_CHECKING option.
Done.
}

{
Next TO-DO item
    Think about the relationship between textures and a shader in RenderObject.
    More specifically when client changes the shader in a RenderObject what should happen to the textures?
    Currently they are just all deleted and client should set textures anew if they need them in the new shader.
    Is this an okay behavior? Is there something else that can be done?

Well, currently we can't really do anything else since we don't have a resource manager.
In the future we might pass an image ID instead of a whole image to setTextureImage(),
so that images are not loaded and unloaded whenever we change textures and/or shaders.
Also we might have a system that keeps track of texture objects existing on the GPU
and reference counting of how many RenderObjects use them, something like that.

It's not the moment to do all that now, so for now we'll keep it as it is:
All textures of a RenderObject will be deleted when a shader is changed
and user will have to set new textures.

Done.
}

----------
09.06.2025
----------

{
Next TO-DO item
    What will happen if you call a create() function of a 2D shape more than once, without calling destroy() in between?

Let's try and see.
Well, currently if I try to call create() on a 2D shape instance that has already been created I get 4 of these warnings in the console
    [WARNING in RenderComponent.cpp:22](Pekan): Creating a render component, but there is already a render component created
        in this RenderComponent instance. Old render component will be destroyed.
and it works fine.

That's ok, it makes some sense, because a 2D shape has an underlying render object which is made up of 4 render components,
and apparently these 4 render components give off the warning.
However, it would be better to catch this incorrect usage while still in the create() function of RenderObject,
so that you get a single warning message saying "RenderObject instance is already created".
Easy enough. Did it.
Now when you try to create() a 2D shape twice you get this warning message
    [WARNING in RenderObject.cpp:27](Pekan): Creating a render object, but there is already a render object
        created in this RenderObject instance.Old render object will be destroyed.

We can do a bit more than that, we can even detect in Shape.
So now if you have a raw RenderObject and you call create() twice on it you will get the warning message above,
but you have a Shape and you call create() twice on it you will get
    [WARNING in Shape.cpp:69](Pekan): Creating a 2D shape, but this Shape instance has been created before, 
        here is a valid render object inside. This old render object will be destroyed.

I also had to add a
    isValid()
function to class Shape. It just checks if the underlying render object is valid.

Tested a few different cases.
Warning messages look good and come up at the right times.
Ignoring the messages, everything else works normally, as expected.
Done.
}

----------
10.06.2025
----------

{
Next TO-DO item is a big one, not by size but by importance.
    Create a 2D camera that can be moved with mouse and/or with WASD keys

For the Camera2D class I have to make a decision if I want to construct a view-projection matrix
from the camera's properties and pass that matrix to the shader,
or just pass camera's properties directly to the shader.
Hmmmm...
I'll use a matrix.
Mainly because it simplifies composition of transforms.

I created a class
    Camera2D
with parameters
    width
    height
    zoom
    position
It maintains a 4D view-projection matrix that can be retrieved by calling
    camera.getViewProjectionMatrix();
and passed to a vertex shader.
Then the vertex shader should multiply vertices by that matrix before passing them to the next shader stage.

How can we test this?
Let's create a new demo. Thinking ahead, let's make it be suitable for testing Batch Rendering as well.
Let's create a scene with thousands of different shapes moving around, and a slider in GUI for controlling the number of shapes.
We'll have 2 goals in this demo:
- To demonstrate how camera can move and zooom in the scene's world
- To optimize the scene with Batch Rendering

So let's create Demo06 starting with a copy of Demo00.
Done.

----------
11.06.2025
----------

    {
    Random side quest.
    I want to be able to control the position where window spawns when an application is launched.
    I think it makes sense for this to be part of struct WindowProperties
        glm::ivec2 initialPosition = glm::ivec2(30, 40);
    and to be used in class Window's create() function like that:
        glfwSetWindowPos(m_glfwWindow, properties.initialPosition.x, properties.initialPosition.y);
    Of course, only if the application is not fullscreen.
    Done.
    It works.
    }

Now let's continue with Demo06.
Created a GUI slider for controlling the number of shapes.
Created a bunch of random rectangles in a coordinate system with a different center and a different scale from the standard [-1, 1].
Currently we can't see them because they are outside of [-1, 1].
We need to use the camera now and configure it so that we can see the rectangle's bounding box properly.

Also added a few new functions under the Pekan::Utils namespace
    int getRandomInt(glm::ivec2 range);
    float getRandomFloat(glm::vec2 range);
    glm::vec2 getRandomVec2(glm::vec2 xRange, glm::vec2 yRange);
    glm::vec2 getRandomIVec2(glm::ivec2 xRange, glm::ivec2 yRange);
just for convenience.

So let's see how we can use the camera now to see the rectangles properly.
We basically need to get the camera's view projection matrix
( We already have a function for that -> Camera::getViewProjectionMatrix() )
and set it as a uniform inside class Shape's shader.
Since all 2D shapes use the same vertex shader, it makes sense that this will apply to all shapes,
so it can be implemented in the base class Shape.
How do we pass the view projection matrix to class Shape? At what point, through what function?
It'd be best to pass the whole camera to Shape::render()
and then Shape::render() can get the view projection matrix and set it to the shader's uniform.
    (
    In the future we might want to have a RenderContext struct/class that holds together the camera, lights, render target,
    and other things that apply to rendering globally.
    For now we'll not do that because we don't need it immediately, it's better to do it when the need is apparent.
    Also another reason - not sure if I want to have 2 different structs RenderContext2D and RenderContext3D
    and this is not a decision that needs to be taken right now.
    )
So let's just have a camera parameter in Shape::render().
    void render(const Camera2D& camera);
For now I'll keep the old render() overload as well so that old demos work as well.
Created a TO-DO item to remove this old render() overload in the future and always provide a camera.
Then we need this new uniform in the vertex shader of class Shape
    uniform mat4 u_viewProjectionMatrix;
and we just need to multiply the position attribute by this matrix before passing it along to the next stage
    gl_Position = u_viewProjectionMatrix * vec4(a_position, 0.0, 1.0);
Now in Shape::render() we can use the camera parameter to get the view projection matrix and set it to the shader's uniform
    const glm::mat4& viewProjectionMatrix = camera.getViewProjectionMatrix();
    m_renderObject.getShader().setUniformMatrix4fv("u_viewProjectionMatrix", viewProjectionMatrix);
That's fundamentally it.

I also added some new methods to Camera2D and a new function getRandomColor() in Pekan::Utils,
and some other minor changes, nothing important.

Tried using a camera in Demo06. Rectangles appear on the window now.
Also tried to move and zoom out the camera in Demo06's update() function.
It works!

Before continuing with other shape types let's allow the camera to be moved by user input.
That's the important thing now. Having different shape types will be needed later when we do batch rendering
to make sure that all shape types can be batched together.

So how do we make camera move with user input?
Well, we can do it as application logic inside of Demo06
by overriding these 2 functions
    bool onMouseMoved(Pekan::MouseMovedEvent& event) override;
    bool onMouseScrolled(Pekan::MouseScrolledEvent& event) override;
in class Demo06_Scene.
Implementing them to move the camera in a standard way is easy, nothing much to say there.
It works like that.

However, this logic should NOT be application logic.
It should be defined in the engine, and the applications should only "enable camera controller" or something like that.

----------
12.06.2025
----------

We have a major design problem now.
It's good to implement this logic for controlling the camera with user input to a separate class
    CameraController2D
and then let the engine directly send events to the currently registered CameraController2D
(there will be only 1 camera controller registered at a time, and users will be able to provide their own controllers)
instead of CameraController2D having to get the events from a layer of the layer stack.
More specifically the function
    _dispatchEvent()    (PekanApplication.cpp)
that currently sends events to all layers of the layer stack,
needs to also send events to the camera controller.
The problem with this is: Where do we put CameraController2D ?
Conceptually it doesn't belong in Renderer because it's not render-specific, it's a user input mechanism.
However, we also can't put it in Core because it needs to know about Camera2D which is a Renderer concept.
(Remember, Core is not supposed to depend on Renderer)
So how can we send events from Core to CameraController2D AND let CameraController2D know about Camera2D?

Well, we need 2 things:
- EventListener class in Core
- A new module, call it
    Tools

This new Tools module will contain the CameraController2D class.
It will depend on both Core and Renderer.
Conceptually it will contain tools that are often used in game development
but not strictly needed.

What about the EventListener class?
It will contain functions
    virtual bool onKeyPressed(KeyPressedEvent& event) { return false; }
    virtual bool onKeyReleased(KeyReleasedEvent& event) { return false; }
    ...
that are currently in class Layer.
We will make class Layer and class CameraController2D both derive from EventListener.
Then, we'll have a mechanism for register/unregister of EventListeners in PekanApplication.
So PekanApplication will have a list of EventListeners
and whenever an event occurs, the function
    _dispatchEvent()
will dispatch that event both to the layer stack AND the registered event listeners.

The Tools module will have a static function for enabling the CameraController2D for a given camera and a given application.
Importantly, there will be only one CameraController2D active at a time.
I think this makes sense, because you basically never want to control more than 1 camera at a time.
Do we need to have multiple instances of CameraController2D? Not really. Data-wise it contains just a pointer to a Camera2D.
If an application has multiple cameras and wants to switch view from one to another, then it will have to just call
    enableCameraController2D(...)
with the newly controlled camera. Internally we'll keep the same CameraController2D instance but just change the Camera2D pointer.

Makes sense.
Let's do it!

Implementation plan:
0. Rename PekanCore to Core and PekanRenderer to Renderer
1. Create class EventListener in Core + register/unregister mechanism + handling in _dispatchEvent()
2. Create Tools module
3. Create class CameraController2D in Tools + enable/disable functionality that registers/unregisters the controller as an event listener
4. Test with 2 different cameras in Demo06, having a key for switching between cameras

Done.
It works!

----------
15.06.2025
----------

Did we accidentially solve this other TO-DO item as well?
    Create a world space and a resolution-independent way to render 2D shapes
Created a square (same width and height in world space) that always sits at the center of the bounding box.
Tried it with different resolutions. It's always a square.
So yes, it works.
Done.
}

----------
16.06.2025
----------

{
Next TO-DO item
    Add support for rotation of 2D shapes

We'll also need to add support for scaling of 2D shapes so let's do it together.

Let's create a new demo
    Demo07
and render a coordinate system in application code.
Later we can move it to engine code.
Done, we have a coordinate system now.

Let's create a rectangle together with position parameter in GUI.
Done.
It works. Looks good.

----------
18.06.2025
----------

Now that we have the demo ready for us, let's implement rotation and scaling in class Shape.

First, I did a small change in how 2D shapes derive from class Shape.
I removed the create() function in Shape - it didn't do anythin beside set default values to some member variables.
Instead I just set them where the member variables are declared, easier.
I also removed the destroy() function and replaced it with this new function
    void destroyRenderObject();
matching the
    void createRenderObject();
function. Now derived classes can provide their own create() and destroy() functions
and call createRenderObject() and destroyRenderObject() from them.

----------
19.06.2025
----------

Let's implement rotation and scale now.
A few hours later... Did it!
What I did in conclusion:
Added these 2 new members in class Shape
    float m_rotation = 0.0f; // in radians
    glm::vec2 m_scale = glm::vec2(1.0f, 1.0f);
together with getters and setters for them
    void setRotation(float rotation); // in radians
    void setScale(glm::vec2 scale);
    inline float getRotation() const { return m_rotation; }
    inline glm::vec2 getScale() const { return m_scale; }
Also added a member which is a transform matrix containing the position, rotation and scale combined in one transform
    glm::mat3 m_transformMatrix = glm::mat3(1.0f);
together with this function
    void updateTransformMatrix();
that updates the transform matrix based on current position, rotation and scale.
This function updateTransformMatrix() is called in each of the setters of position, rotation and scale.
Then derived classes can use the transform matrix to transform vertices from local space to world space.
How exactly will they do this?
Well, I introduced this new pure virtual function
    virtual void updateTransformedVertices() = 0;
The idea is that derived classes will have 2 copies of the vertices, one in local space and one in world space.
Whenever something about the shape in local space changes, for example width or height of a rectangle, the local vertices will be changed,
and the world vertices will have to be changed as well to reflect that.
Whenever the position, rotation or scale of a shape is changed, the local vertices remain the same,
but the world vertices need to be updated with the new transform matrix.
That's when this function updateTransformedVertices() will be called automatically from Shape.
Basically whenever the transform matrix is updated, this function updateTransformedVertices() gets called
to update the world space vertices.
Each derived class should implement its own way of keeping the 2 copies of the vertices,
and its own way of updating the world vertices in updateTransformedVertices(), that's why the function is pure virtual.
Obviously, the vertices that end up on the GPU will be the world vertices, so this is the "official" vertex data of a shape.
It's the one that needs to be returned from getVertexData(),
and that's why at the end of each implementation of updateTransformedVertices() we need to call
    Shape::updateRenderObject();
to update the underlying render object, because we just changed the vertex data.
That's about it.

In Demo07 I tested the position, rotation and scale of each shape type.
Everything works, except LineShape.
There is a weird bug with it - it appears as a very thin isosceles triangle, instead of a rectangle.
Not sure why.

After a lot of debugging, and asking ChatGPT, I found the bug.
It's a very sneaky one.
Basically the problem is in RenderObject's setIndexData() function.
The way it currently works is this:
    void RenderObject::setIndexData(const void* data, long long size, BufferDataUsage dataUsage)
    {
        PK_ASSERT_QUICK(m_indexBuffer.isValid());

        m_indexBuffer.setData(data, size, dataUsage);
        m_indexDataUsage = dataUsage;
    }
This line
    m_indexBuffer.setData(data, size, dataUsage);
calls IndexBuffer's setData() function, which binds the index buffer and sets the data.
However, binding an index buffer is relative to the currently bound vertex array.
And in Demo07 it so happens that the currently bound vertex array is LineShape's one,
so the line
    m_triangle.setPosition(...);
leads to modifying m_line's index buffer, if done immediately after doing something with m_line, which is absurd.

The solution is, of course, to just bind the correct vertex array before this line
    m_indexBuffer.setData(data, size, dataUsage);
So RenderObject's setIndexData() function now becomes this:
    void RenderObject::setIndexData(const void* data, long long size, BufferDataUsage dataUsage)
    {
        PK_ASSERT_QUICK(m_indexBuffer.isValid());

        m_vertexArray.bind();
        m_indexBuffer.setData(data, size, dataUsage);
        m_indexDataUsage = dataUsage;
    }
For the exactly same reason we should do the same in these functions:
    setVertexData
    setVertexSubData
    setIndexSubData
basically binding the vertex array before using the vertex buffer or the index buffer.

With that, we are done with Shape's rotation and scale.
}

----------
20.06.2025
----------

{
Let's look at this TO-DO item
    Think about maybe removing the overload of Shape::render() that doesn't take in a camera parameter

I actually think it's better to leave this overload.
It's perfectly valid to not use a camera and just render on the default [-1, 1] range.
It could even be a bit faster, if you don't have any camera movements.
So, leaving it for now.
}

----------
21.06.2025
----------

{
Let's do this TO-DO item
    Make CameraController2D zoom at the direction of the mouse instead of towards the center

We basically need to keep track of mouse's position in world space before zooming,
and mouse's position in world space after zooming,
and then use the difference between the two to move the camera.
This will ensure that the world position at the mouse will remain the same,
effectively "zooming towards it".

For that to work, we'll need a helper function in Camera2D
    glm::vec2 screenToWorld(glm::vec2 screenPos) const;
that converts a given position from screen space to world space.

Then in CameraController2D's onMouseScrolled() function we can do this before applying the zoom
    const glm::vec2 mousePosWorldBefore = camera->screenToWorld(m_mousePos);
and this after applying the zoom
    const glm::vec2 mousePosWorldAfter = camera->screenToWorld(m_mousePos);
and then use the difference between those two
    const glm::vec2 cameraPosDelta = mousePosWorldBefore - mousePosWorldAfter;
to move the camera
    camera->move(cameraPosDelta);

That's it.
Looks pretty good. Works fine in both Demo06 and Demo07.
Done.
}

{
Let look at this TO-DO item now
    Fix bug with CameraController2D receiving events from ImGui layer.
    For example, when you move an ImGui slider left and right, the camera behind it also moves left and right.

First of all, I noticed that this bug happens in Demo06 but NOT in Demo07.
Why could that be?
Well, it's because in Demo07 the ImGui window is bigger (higher) than the GLFW window,
so it cannot be fully contained inside.
It's when the ImGui window is fully contained inside the GLFW window,
when GLFW, and hence Pekan, starts receiving events from ImGui.
A bit weird, but yeah, keep it in mind.

So, obviously if the ImGui window is outside of the main window, there is no problem.
Let's look at the case when the ImGui window is fully contained inside the main window.

ImGui reacts to events in its own way, so it DOES work properly - all the sliders, buttons, etc. work fine.
The problem is only that ImGui does NOT inform Pekan that it has handled an event.
We can solve this by overriding all "on event" functions in PekanGUIWindow (except window ones)
and checking if ImGui wants to consume this event.
Luckily, there are functions inside of ImGui for checking exactly that.
So let's implement it.

Done.
Demo06 works fine now.
}

{
Let's quickly do this TO-DO item as well
    Add overload of Camera2D's setSize() function that takes in a single float - the scale,
    and automatically calculates width and height with the same ratio as window's resolution.

Pretty easy, nothing to say.
Done.

By the way, I noticed that 2 of the demos don't work fully correctly regarding their rectangles,
because recently, when we added rotation and scale to all shapes, we changed the center of all rectangles
to be at their actual center, and not at their lower-left corner.
So now I just changed the 2 demos to account for the fact that a rectangle's position is its center.
Done.
}

{
There is one final TO-DO item in the list, so let's do it!
    Implement triangulation algorithm in PolygonShape to support non-convex polygons

First, let's create an example concave polygon and make it quite complicated, in Demo04.
Done.
It's not rendered correctly, as expected.

Before we proceed to implementing the algorithm let's create new files
    MathUtils.h
    MathUtils.cpp
in Core with some helper functions for maths and 2D geometry.
For now I can see myself needing these 3 functions
    bool isPointInTriangle(glm::vec2 p, glm::vec2 a, glm::vec2 b, glm::vec2 c);
    bool isOrientationCCW(glm::vec2 a, glm::vec2 b, glm::vec2 c);
    float getDeterminant(glm::vec2 a, glm::vec2 b, glm::vec2 c);

Time to implement the actual triangulation.
Implemented the "Ear Clipping" algorithm in MathUtils in this new function
    bool triangulatePolygon(const std::vector<glm::vec2>& vertices, std::vector<unsigned>& indices);
Not going to go into details of the actual implementation, it's pretty standard.
The important thing is that you can give it a list of 2D vertices,
and it will give you back a list of indices into the vertices, forming a bunch of triangles
that combine to the shape of the whole polygon.

How can we use that in PolygonShape?
Fairly straightforward, we just need to use an IndexBuffer in the underlying render object,
so to do that we can have a new member variable
    std::vector<unsigned> m_indices;
that will contain the indices, and then to enable the use of indices we need to override this function
    const unsigned* getIndexData() const override { return m_indices.data(); }
to return a pointer to the indices data.
Then obviously we can fill this indices vector with the correct indices
by calling the new triangulation function from MathUtils, like that:
    MathUtils::triangulatePolygon(m_verticesLocal, m_indices)
I'll have a new function in PolygonShape that wraps this call, together with error checking, and clearing m_indices before the call
    void PolygonShape::triangulate()
    {
        m_indices.clear();
        if (!MathUtils::triangulatePolygon(m_verticesLocal, m_indices))
        {
            PK_LOG_ERROR("Failed to triangulate a polygon. It's probably a badly defined polygon, possibly self-intersecting.", "Pekan");
        }
    }
So now we just need to call this triangulate() function every time that m_verticesLocal changes, which happens in
    create()
    setVertices()
    setVertex()

----------
22.06.2025
----------

Another thing, the triangulatePolygon() function in MathUtils expects the vertices in CCW order,
so in PolygonShape we can check if the vertices trace the polygon in CCW order,
and if not, we can reverse them
    if (!MathUtils::isPolygonCCW(m_verticesLocal))
    {
        std::reverse(m_verticesLocal.begin(), m_verticesLocal.end());
    }
where this isPolygonCCW() function is also a new function in MathUtils,
checking if the vertices form a CCW polygon.
NOTE: This does NOT mean that every triplet of vertices turns CCW.
      If the polygon is concave, there will definitely be triplets of CCW and triplets of CW.
      What we need is the total turning of all vertices to be in CCW.

That's pretty much it.
Oh one more thing, in case of a convex polygon we will NOT use the Ear Clipping algorithm for triangulation,
we don't need to do triangulation manually, we can just use a triangle fan primitive as before.
(Ear Clipping is not very fast for big polygons (it's O(n^2)) so it's better to avoid it if we don't need it)
So whenever m_verticesLocal is changed we also need to check if the new polygon is convex.
If it is, we can keep indices empty, and then react to empty indices in
    getIndexData()
    getDrawMode()
as follows
    const unsigned* getIndexData() const override
    {
        // If indices are empty, this means that polygon is convex,
        // so it didn't need triangulation - that's why we don't have indices.
        if (m_indices.empty())
        {
            return nullptr;
        }
        return m_indices.data();
    }

    DrawMode getDrawMode() const override
    {
        // If indices are empty, this means that polygon is convex,
        // so use triangle fan primitive.
        if (m_indices.empty())
        {
            return DrawMode::TriangleFan;
        }
        // Otherwise use triangle primitive + indices
        return DrawMode::Triangles;
    }
so that if polygon is convex, we return NULL index data and we return DrawMode::TriangleFan as the primitive.
Otherwise if polygon is concave, we triangulate it and use indices, so we return non-null index data
and we return DrawMode::Triangle as the primitive.
I encapsulated all this logic in a new function in PolygonShape
    void handleNewVerticesLocal();
so basically this function needs to be called whenever m_verticesLocal changes.
That's it.

It works!
Both the convex and concave polygons in Demo04 are rendered correctly.
Done.
}

----------
23.06.2025
----------

{
Let's do a small side quest.
Allowing control of zoom speed in CameraController2D.
Created this new function in PekanTools
    static void setCameraController2DZoomSpeed(float zoomSpeed);
What it does is it calls this new function of CameraController2D
    s_cameraController2D->setZoomSpeed(zoomSpeed);
    inline void setZoomSpeed(float zoomSpeed) { m_zoomSpeed = zoomSpeed; }
that just sets this new member variable of CameraController2D
    float m_zoomSpeed = DEFAULT_CAMERA_CONTROLLER_2D_ZOOM_SPEED;
which is set by default to the default value of the zoom level which is
    static const float DEFAULT_CAMERA_CONTROLLER_2D_ZOOM_SPEED = 1.1f;

In Demo06 I want the zoom to happen slower, so we can test the new function there like that
    PekanTools::setCameraController2DZoomSpeed(1.05f);
It works!
Done.
}

{
All TO-DO items are done now.
We have 1 more feature to complete v0.2 and that is
    Batch Rendering

This is probably the most complicated feature so far.

Before starting to implement batch rendering, let's first finish Demo06.
I've been meaning to generate all types of shapes there, not just rectangles.
So let's do that now.

Okay, finished work on Demo06.
It now contains all shape types. All of them move, rotate and scale randomly, and have random properties.
I also added checkboxes in GUI for hiding each shape type.
It looks good, and FPS very quickly drops when you increase the number of shapes.
I think it's a perfect target for Batch Rendering optimization.

----------
24.06.2025
----------

Let's start with Batch Rendering.
First, I will need a new class called
    Renderer2D
but I already have a class
    PekanRenderer
so it becomes a bit confusing.
I thought about it for a while and I think it's better to separate PekanRenderer into 2 new classes
    RenderState
    RenderCommands
First one will contain functions that change the global rendering state, for example
    enableBlending()
    enableMultisampleAntiAliasing()
    etc.
Second one will contain render commands that directly translate to a GPU operation. In Pekan's case only
    draw()
    clear()
...Separated them. All functions that were previously in PekanRenderer are now in RenderState and RenderCommands.

Now let's create the new class
    Renderer2D
in files
    Renderer2D.h
    Renderer2D.cpp
under
    src/Renderer/2D

For now we'll focus only on batch rendering shapes.
Later we'll think about textures, custom geometries, custom shaders, etc.

Renderer2D will be a static class.
It will keep track of a "batch" containing vertices and indices of many shapes at once.
This "batch" will be just a render object
    static RenderObject s_batch;
Then, adding a shape to the batch will happen when user requests to render a shape.
So let's create a render function in Renderer2D
    static void render(const Shape& shape);
This function will not directly render the shape. It will just add it to the batch.
The actual rendering of the batch will happen when all shapes are added to the batch.
How do we know when that is?
Well, not sure if there is a better way but for now I'll add these 2 functions in Renderer2D
    static void beginFrame();
    static void endFrame();
that should be called at the beginning and end of application's render() function.
Can't we let the engine do that automatically?
Yes, we should definitely do that in the future, but now it's not so straightforward to do it
because with current architecture it should be done in PekanApplication
but PekanApplication is in Core so it doesn't know about Renderer2D.
So, for now, clients will have to manually call
    Renderer2D::beginFrame();
    ...
    Renderer2D::endFrame();
in their render() function.
The idea here is that beginFrame() will clear the batch, and endFrame() will render the batch.
Between them shapes can be added to the batch with
    Renderer2D::render(...)
Also, currently each shape is drawn separately and the camera's view projection matrix is set as a uniform
inside the shader each time. We don't need to do that anymore.
We'll need to set this view projection matrix only once when we render the batch,
so we'll need a new way of specifying a camera.
Let's have a single static camera pointer inside of Renderer2D
    static CameraWeakPtr s_camera;
and a function for setting a camera
    static inline void setCamera(const CameraSharedPtr& camera) { s_camera = camera; }
Then in endFrame() when we draw the batch we'll set shader's uniform to the camera's view projection matrix.
That's the general idea of Renderer2D for now.

In order for all of this to work, we'll have to do some refactoring in class Shape and its derived classes.
Specifically, we need every shape to be able to give Renderer2D its vertices and indices.
Shape classes already have
    virtual const glm::vec2* getVertexData() const = 0;
    virtual const unsigned* getIndexData() const { return nullptr; }
but now we'll need a vertex to contain position and color, not just position.
That's because until now we used to set a shape's color to a uniform inside the shader,
but we can't do that anymore because all shapes are drawn with a single draw call.
(
    We could actually do this with a uniform array or a 1D texture,
    and it would probably be a better idea.
    Created a TO-DO item for it.
)
So let's create a small struct for holding together the data of a single shape vertex
    struct ShapeVertex
    {
        glm::vec2 position = glm::vec2(0.0f, 0.0f);
        glm::vec4 color = glm::vec4(1.0f, 1.0f, 1.0f, 1.0f);
    };
and now the function
    getVertexData()
will need to return a ShapeVertex array
    virtual const ShapeVertex* getVertexData() const = 0;

I want to rename and change these 4 functions a bit
    virtual const ShapeVertex* getVertexData() const = 0;
    virtual int getNumberOfVertices() const = 0;
    virtual const unsigned* getIndexData() const { return nullptr; }
    inline int getIndexDataSize() const;
to be like that
    virtual const ShapeVertex* getVertices() const = 0;
    virtual int getVerticesCount() const = 0;
    virtual const unsigned* getIndices() const { return nullptr; }
    virtual int getIndicesCount() const { return 0; }

Another thing. Obviously, we don't need an "underlying render object" for each shape anymore,
so we can remove all the logic surrounding that and the actual m_renderObject member as well.

Then I did some changes on WHEN things are calculated.
Specifically, the transform matrix in base class Shape does NOT need to be computed
until it is required by a derived class. Once it has been computed we don't need to recompute it
unless position, scale or rotation is changed.
To implement this behavior I used a flag
    mutable bool m_needUpdateTransformMatrix = false;
Similarly, in derived classes we don't need to calculate local vertices until they are needed.
For example, in RectangleShape we can just keep the width and height and NOT have any vertices at all,
until they are needed. When they are needed we will compute them.
Then, we don't need to recompute them unless width and height are changed.
To implement this behavior I used a flag
    mutable bool m_needUpdateVerticesLocal = true;
in all derived classes.
Similarly, world vertices can be computed when they are needed, and they don't need to recomputed
unless transform is changed in base class Shape ot properties are changed in the derived class.
To implement this behavior I used a flag
    mutable bool m_needUpdateVerticesWorld = true;

Did a few other small things.
It was a biiiiig refactor. Almost would've been easier to start over with shapes :D
But that's okay.
Now it looks pretty good, stable and optimized.

To sum up, the high-level design of shapes for batch rendering is the following:
- We have a base class Shape that keeps track of a shape's transform (position, rotation, scale)
  and provides API for getting vertices (in world space) and indices.
  It (re)calculates the transform matrix only when needed.
- We have a few derived classes implementing a specific shape type (rectangle, circle, etc.)
  Each of them consists of some properties that user can configure (e.g. width and height of a rectangle).
  Local vertices are (re)calculated from properties only when needed.
  World vertices are (re)calculated from local vertices using the transform matrix, only when needed
  Indices are either constant, or (re)calculated from properties only when needed.
- Renderer2D only cares about getting vertices (in world space) and indices from a shape.
  How a shape is represented, what properties it has, at what point its local vertices are (re)calculated,
  at what point its world vertices are (re)calculated - are all internal details of shapes,
  they are not essential to how Renderer2D works, and how batch rendering works.

So until now, we've literally done zero batch rendering.
We only refactored shape classes so that they are convenient and make sense for batch rendering,
and we have a skeleton for Renderer2D.

----------
28.06.2025
----------

I realized we can't use a triangle fan primitive for batch rendering,
so I converted all shape types that use triangle fan to use triangles instead.

I also noticed that the function
    getIndices()
in classes derived from Shape returns the indices data
without making sure that it has been updated.
In some shape types the indices data is updated together with local vertices.
So before returning the indices data we need to check if
    m_needUpdateVerticesLocal
is true, and if so, call updateVerticesLocal().
Or even better (for some shape types) could be to have a new flag
    mutable bool m_needUpdateIndices = true;
and a new function
    void updateIndices() const;
and then in getIndices() we can check if m_needUpdateIndices, and call updateIndices(), before returning the actual indices data.
In class
    CircleShapeStatic
we don't need to do that because its indices will stay the same for the whole lifetime of an instance.
(because the number of segments doesn't change)
In class
    PolygonShape
we also don't need this because local vertices are updated very much together with indices.
It doesn't make much sense to update them separately, so in getIndices() we can just do this
    if (m_needUpdateVerticesLocal)
    {
        updateVerticesLocal();
    }
Actually, class
    CircleShape
is the only one that can benefit from having a separate flag m_needUpdateIndices and a separate function updateIndices().

----------
29.06.2025
----------

We are finally ready to implement batch rendering.
The way I'll do it is to have 2 vectors keeping all vertices and all indices of the shapes being rendered
    static std::vector<ShapeVertex> s_vertices;
    static std::vector<unsigned> s_indices;

In
    Renderer2D::render(const Shape& shape)
(which is called when a shape is requested for rendering)
we'll do that
    const ShapeVertex* vertices = shape.getVertices();
    const int verticesCount = shape.getVerticesCount();
    const unsigned* localIndices = shape.getIndices();
    const int indicesCount = shape.getIndicesCount();
to get the vertices and indices of the shape.
Then we'll just add the shape's vertices to the vertices list, easily like that
    s_vertices.insert(s_vertices.end(), vertices, vertices + verticesCount);
To add the indices to the indices list it won't be that simple,
because we actually need to offset the indices before adding them to the list,
so that they correctly reference the vertices in the vertices list.
Because, you see, in a shape the indices are starting from 0 because the vertices are also starting from 0.
However, in the batch, the indices need to reference the same vertices they are referencing in the shape
but now these vertices have changed their indices, they no longer start at 0.
TL;DR we need to add s_vertices.size() (size before adding the new vertices) to each index before adding it to the indices list.
To do that we'll cache the size of vertices list before adding the new vertices
    const unsigned oldVerticesSize = unsigned(s_vertices.size());
We'll also cache the old size of indices list
    const size_t oldIndicesSize = s_indices.size();
(they are different types intentionally, not important)
Then we'll just add each index + oldVertexSize to the indices list
    for (size_t i = oldIndicesSize; i < s_indices.size(); i++)
    {
        s_indices[i] = localIndices[i - oldIndicesSize] + oldVerticesSize;
    }
That's it.
Now we have the 2 lists in tact. They should correctly contain vertices and indices of batched shapes.
    s_vertices
    s_indices

At the beginning of every frame, so in
    Renderer2D::beginFrame()
we'll clear the 2 lists
    s_vertices.clear();
    s_indices.clear();
from vertices and indices of the previous frame.

At the end of every frame, so in
    Renderer2D::endFrame()
we'll have to actually create the batch which is a render object
    static RenderObject s_batch;
and fill it with the vertices and indices from the 2 lists, and draw it.
However, we don't need to create() and destroy() the batch on each frame.
It'd be better to create() the batch once at the beginning of an application
and destroy() it once at the end of an application.
Then in endFrame() we can only update vertex data and index data in the already created batch.
For that I will create functions
    static void init();
    static void exit();
in Renderer2D that should be called when application is initialized and when application is exited.
Ideally, these would be called automatically by the engine, but for now we'll require user to call them
in the init() and exit() functions of their scenes. (If they have multiple scenes it would become messy)
Same reason we had for beginFrame() and endFrame() - at some point they will be called by engine code,
but for now we can't do that because it would need to happen in PekanApplication which is in module Core
which does NOT know about Renderer2D.
So,
what will the 2 functions init() and exit() do exactly?
Well, init() will create the batch with empty vertices and empty indices,
but configuring the VertexBufferLayout and the 2 shaders
    s_batch.create
    (
        s_vertices.data(),
        0,
        { { ShaderDataType::Float2, "position" }, { ShaderDataType::Float4, "color" } },
        BufferDataUsage::DynamicDraw,
        FileUtils::readFileToString(VERTEX_SHADER_FILEPATH).c_str(),
        FileUtils::readFileToString(FRAGMENT_SHADER_FILEPATH).c_str()
    );
    s_batch.setIndexData(s_indices.data(), 0, BufferDataUsage::DynamicDraw);
basically making sure that we have a batch that is ready to receive vertices and indices and then be drawn.
Then obviously in exit() we'll make sure to destroy() the batch. We can also clear the 2 lists just in case
    s_batch.destroy();
    s_vertices.clear();
    s_indices.clear();
So now, we can finally get to the point - what happens in endFrame().
We'll set batch's vertex data and index data to the data from the 2 lists that we've gathered from Renderer2D::render() calls
    s_batch.setVertexData(s_vertices.data(), s_vertices.size() * sizeof(ShapeVertex));
    s_batch.setIndexData(s_indices.data(), s_indices.size() * sizeof(unsigned));
We will also set shader's view projection matrix uniform to the camera's view projection matrix
    const glm::mat4& viewProjectionMatrix = camera->getViewProjectionMatrix();
    s_batch.getShader().setUniformMatrix4fv("u_viewProjectionMatrix", viewProjectionMatrix);
That is if we have a camera at all
    if (camera != nullptr)
Otherwise we'll just set a default view projection matrix (a uniform matrix)
    static const glm::mat4 defaultViewProjectionMatrix = glm::mat4(1.0f);
    s_batch.getShader().setUniformMatrix4fv("u_viewProjectionMatrix", defaultViewProjectionMatrix);
And finally we'll do a draw call, drawing all indices
    RenderCommands::drawIndexed(s_indices.size());

Then to sum up what should be done in client code to make it work:
1.  Inside of your scene's init() function you should call
        Renderer2D::init();
    (If you have multiple scenes do this in only one of them)
2.  Inside of your scene's exit() function you should call
        Renderer2D::exit();
    (If you have multiple scenes do this in only one of them)
3.  At the beginning of your scene's render() function you should call
        Renderer2D::beginFrame();
    (If you have multiple scenes do this in each one of them)
4.  At the end of your scene's render() function you should call
        Renderer2D::endFrame();
    (If you have multiple scenes do this in each one of them)
5.  In your scene you can have as many shapes as you want
    and you can render them like that:
        m_rectangle.render();
6.  In your scene you can have a Camera2D like that:
        using CameraPtr = std::shared_ptr<Pekan::Renderer::Camera2D>;
        ...
        CameraPtr m_camera;
    and you need to set it as "the camera" in Renderer2D like that:
        Renderer2D::setCamera(m_camera);
        (in your scene's init() function, after creating camera)
    (Not related to changes here, but) if you want to control your camera with the mouse you can do this:
        PekanTools::enableCameraController2D(m_camera);
        (in your scene's init() function, after creating camera)

That's it.
Batch rendering works now!

It massively increased performance of Demo06.
Without batch rendering, Demo06 with 6k shapes works at 15 FPS.
With batch rendering, Demo06 with 40k shapes works at 60 FPS.

Calling it done here,
BUT we need to cleanup code, maybe restructure a bit, and do the 3 TO-DO items that emerged.
}

----------
03.07.2025
----------

{
Let's do some cleaning up today.
I will start with automating the initialization and exiting of Renderer2D,
so that client code doesn't have to manually call Renderer2D::init() and Renderer2D::exit().

To do this, I will introduce this new concept in Pekan - a "subsystem".
Conceptually a subsystem will be a building block of Pekan, which:
- Is self-contained.
    (Changes in one subsystem don't require changes in another)
    (Can be easily plugged into another engine)
- Is not part of the "core" engine.
    (It makes sense to use the engine without it)

In code, a subsystem will be just anything that needs to be initialized when engine is intialized,
and exited when engine is exited.

So I added this simple interface
    class ISubsystem
    {
        friend class SubsystemManager;

    public:
        virtual ~ISubsystem() = default;
    private:
        virtual void init() = 0;
        virtual void exit() = 0;
    };
( init() and exit() are private and class SubsystemManager is a friend to make sure that only SubsystemManager can call init() and exit() on a subsytem )
in a new file in Core
    ISubsystem.h
together with this static class
    class SubsystemManager
in new files in Core
    SubsystemManager.h
    SubsystemManager.cpp
What this SubsystemManager does is it allows subsystems to register themselves in Pekan
    static void registerSubsystem(ISubsystem* subsystem);
and allows Pekan to initialize all registered subsystems and exit all registered subsystems
    static void initAll();
    static void exitAll();
The actual list of registered subsystems is a global static variable in
    SubsystemManager.cpp
and it looks like this
    static std::vector<ISubsystem*> g_subsystems;
That's pretty much it.

Then, our first subsystem will be Renderer2D.
Let's make it derive from ISubsystem
    class Renderer2D : public ISubsystem
and change its 2 functions
    static void init();
    static void exit();
from being static to non-static and put "override" to make sure that they override the init() and exit() from ISubsystem
    void init() override;
    void exit() override;

To register Renderer2D as a subsystem we'll have to first have an instance of it.
Let's have a global static instance in Renderer2D.cpp
    static Renderer2D g_Renderer2D;
and we'll register it statically with a lambda, like that:
    static bool s_registered = []()
    {
        SubsystemManager::registerSubsystem(&g_Renderer2D);
        return true;
    }();
That's it.

Now, at what point should we initialize all subsystems and exit all subsystems?
It makes sense to do it in PekanEngine's init() and exit() functions.
However, this does NOT work currently, because to initialize Renderer2D we need a valid OpenGL context,
which is obtained when we create the window. The window is not created in PekanEngine's init() function,
it's created manually by client code calling PekanEngine::createWindow() in their application's _init() function.
That's not good.
We need some redesign of the core architecture.
For now, what I'll do as a workaround is to initialize subsystems in PekanApplication instead of PekanEngine,
and do it, importantly, after calling _init() so after the window has been created.
    SubsystemManager::initAll();
Exiting could technically be done in PekanEngine, but just for consistency let's also do it in PekanApplication
    SubsystemManager::exitAll();
That's it.
It works now.
Renderer2D is automatically initialized and exited when application is initialized and exited.

Created a TO-DO item for core's redesign.
}

----------
05.07.2025
----------

{
Let's immediately start with the TO-DO item described above.
We need to redesign the core architecture a little bit.

First of all, we'll separate the Renderer module into 2 modules
    Graphics
    Renderer
The Graphics module will consist only of abstractions over GPU objects and concepts.
The Renderer module will use Graphics as a building block to build 2D shapes, 3D objects, cameras, etc.
Done, separated them.
Everything works as before, but now we have a new module
    Graphics

Now, the hard part.
We need to move all OpenGL related code to Graphics.
Core needs to be agnostic of the graphics API.
To do that I'll create a Graphics subsystem in the new Graphics module, in new files
    Graphics.h
    Graphics.cpp
It will register itself in SubsystemManager the same way that Renderer2D does it.
The main thing it will do is load OpenGL upon initialization.
Okay, not too hard, got it working after a while.

There is a problem now. The Renderer2D subsystem must always be initialized AFTER the Graphics subsystem.
Currently we can't ensure that, because they both get registered statically in their own modules,
we can't know which one will be the first to register itself.
To handle this we'll add this function to ISubsystem
    virtual ISubsystem* getParent() { return nullptr; }
that can be overridden by a specific subsystem to return its "parent" subsystem,
meaning a subsystem that needs to be initialized BEFORE the given subsystem.
If a subsystem does not implement this, it will simply be a child to Core,
meaning that it can be initialized whenever.
We'll use that to say that Renderer2D's parent must be Graphics.
In Renderer2D do this:
    ISubsystem* getParent() override;
    ...
    ISubsystem* Renderer2D::getParent()
    {
        return Graphics::Graphics::getInstance();
    }
where this getInstance() function is a function in the Graphics subsystem:
    Graphics* Graphics::getInstance()
    {
        return &g_graphics;
    }
returning a pointer to the global Graphics instance.
Then finally, in SubsystemManager's initAll() function we can initialize all subsystems but in order from parent to leafs.
Basically, for each subsystem X recursively initialize parents until there is no parent left, and then initialize X.
That's it. Now we're sure that Graphics will be initialized before Renderer2D.
It works!

Next step is to create the window when engine is initialized instead of giving control to user to do it whenever they want.
This is important, because we need the window at initialization time in order to load OpenGL
in order to initialize subsystems like Graphics and Renderer2D.
Okay, did it.
What I did is added this new virtual function to class PekanApplication
    virtual ApplicationProperties getProperties() const { return {}; }
returning ApplicationProperties which is a new struct
    struct ApplicationProperties
	{
		WindowProperties windowProperties;
	};
for now only containing window properties, in the future it might contain other stuff as well.
Then in PekanEngine, instead of having a function createWindow() we'll always create a window in the init() function
using the window properties retrieved from calling application's getProperties()
    const ApplicationProperties properties = application->getProperties();
    if (!s_window.create(properties.windowProperties))
    {
        return false;
    }
Together with that we'll also initialize ImGui right after creating the window, of course.
To match this behavior, we can remove the isWindowCreated flag because now we'll always have a window created,
and in the exit() function we can just directly destroy window and exit ImGui.
That's pretty much it.
Then, of course, all applications that want to have specific window properties will have to override the getProperties() function, like that:
    Pekan::ApplicationProperties getProperties() const override;
    ...
    ApplicationProperties Demo00_Application::getProperties() const
    {
        ApplicationProperties props;
        props.windowProperties.title = getName();
        return props;
    }

Now that we have this new mechanism of creating the window, we can initialize all subsystems in PekanEngine
instead of PekanApplication, because we are sure that we have a window, and hence a valid OpenGL context.
So in PekanEngine's init() function, right after creating the window and initializing ImGui, we can do this
    SubsystemManager::initAll();
and in PekanEngine's exit() function, after destroying the window and exiting ImGui (not necessary), we can do this
    SubsystemManager::exitAll();

That's it. It works!
And, of course, it's much better structuered now.

But wait, there is a bug. Some demos crash.
(It turned out the bug is not related to these last changes, but the changes before that)
They crash because apparently OpenGL is not loaded.
It's not loaded because the Graphics subsystem is not registered.
Why is that?
Well, currently we are registering the Graphics subsystem in Graphics.cpp like that
    static bool s_registered = []()
	{
		SubsystemManager::registerSubsystem(&g_graphics);
		return true;
	}();
with a static lambda, executed immediately.
However, this does NOT work if we are not using any of the symbols in this .cpp file.
The linker just ignores the .obj file generated for this .cpp file if we don't use any of the symbols.
In some demos we don't use any of the symbols from Graphics.h and that's why we get the crash.
The solution is to NOT do this static lambda thing.
It's kind of weird, even if it worked.
Soooo, what is a better way to do it?
Well, I considered a few different ways, but at the end I decided to add this new static function to class Graphics
    static void registerSubsystem();
It will just register the subsystem to SubsystemManager
    void Graphics::registerSubsystem()
	{
		SubsystemManager::registerSubsystem(&g_graphics);
	}
The question is who will call it?
Well, I'll let the client code call it.
Probably not the most user-friendly solution, but I'll let every subsystem define a macro for "including it in Pekan"
    // Use this macro in your application's main function to include the Graphics subsystem of Pekan
    #define PEKAN_INCLUDE_SUBSYSTEM_GRAPHICS Pekan::Graphics::Graphics::registerSubsystem()
and users will have to list the subsystems that they want to use, in their main function.
That's fair, to be honest, in a finished game engine it would be normal to require from user
to explicitly "include the Graphics subsystem" if they want graphics in their game,
or "include the Networking subsystem" if they want networking in their game.
This might be done with checkboxes in GUI when creating your projects, and/or in project settings.
Of course, we might want some subsystems to be included by default, like Graphics for example.
But for now we'll not worry about that.
For now requiring that client code include the subsystems that it needs, is an okay solution.
So, TL;DR client code will have to do something like that in their main function
    #include "Graphics.h"
    #include "Renderer2D.h"
    ...
    int main()
    {
        PEKAN_INCLUDE_SUBSYSTEM_GRAPHICS;
        PEKAN_INCLUDE_SUBSYSTEM_RENDERER2D;
        ...
    }
Of course, did the same thing for Renderer2D. They have to have matching interfaces.
Okaaaay, it works now. The bug is fixed.
}

----------
07.07.2025
----------

{
Started to just look around in code to make sure everything looks good, especially in Core.
Found a small thing that annoys me in
    LayerStack
Why do we use raw pointers there?
    std::vector<Layer*> m_layers;
Looked at old notes from when I was implementing this, and there is no mention of the raw pointers.
We want LayerStack to own the layers but we also don't want to necessarily delete the layers when deleting the layer stack.
They might be used somewhere else for debugging or whatever.
Simply, use shared_ptr instead.
    typedef std::shared_ptr<Layer> LayerPtr;
    ...
    std::vector<LayerPtr> m_layers;
Then we don't need the destructor at all, and everything is simpler and safer.
(Performance would not be an issue, because layers will always be just a few. At most 100 in a very extreme situation. A 100 is still nothing for performance)

Of course, in order for this to work, we must change all places that use the layer stack
to use shared pointers.
Done.
Everything works as before.
}

{
Still looking around.
I see that this function in PekanApplication
    inline void setFPS(double fps) { m_fps = fps; }
doesn't really need to be a function.
It would be better to have the FPS as a member of ApplicationProperties.
Let's do that now.
    struct ApplicationProperties
    {
        WindowProperties windowProperties;
        double fps = 0.0;
    };
and then in PekanApplication's run() function we'll just do
    const double fps = getProperties().fps;
to get the FPS.
The way applications will set their FPS now will be in their getProperties() function, like that
    ApplicationProperties props;
    props.fps = 60.0;
    ...
    return props;
That's it. It works.
}

{
Still looking around.
I see another thing that annoys me.
Why do we need this function in class Layer?
    void stopRunningApplication();
We already have a protected member
    PekanApplication* m_application = nullptr;
so we should be able to call
    m_application->stopRunning();
The reason I created this function in the first place was because I made PekanApplication's stopRunning() function private
and then made class Layer a friend to class PekanApplication so that only class Layer can stop running the application,
but that's not necessary.
We can easily have
    stopRunning()
be public. It's okay that an application can be stopped from any code.
Then we don't need this middle-man function
    stopRunningApplication()
in class Layer and we don't need to make Layer and PekanApplication be friends.
Easy.
}

{
Still looking around.
I see another thing.
This function
    virtual bool _fillLayerStack(LayerStack& layerStack) = 0;
in PekanApplication doesn't make much sense.
It doesn't make much sense the fact that an "init" function has to specifically fill a layer stack,
and also might do other initialization code that the derived application might need.
It would be cleaner to separate it into 2 functions like that
    virtual bool _init() { return true; }   
    virtual bool _fillLayerStack(LayerStack& layerStack) = 0;
A derived application definitely has to fill the layer stack and they can do it in
    _fillLayerStack()
They might or might not have specific initialization logic. (That's why _init() is not pure virtual)
If they do, they can override
    _init()
All of my demos do NOT have initialization logic so they will not override _init().
They only need to implement _fillLayerStack() and they already do, they just call it _init(),
so we just have to rename _init() to _fillLayerStack() in all demos.
That's it. It works. Looks better.
}

----------
08.07.2025
----------

{
Looking around again today.
I see a small thing about FPS.
It's not very nice that we use FPS of 0 to indicate "use VSync",
instead of having a useVSync member in ApplicationProperties.
So let's do that.
Add "useVSync" member to ApplicationProperties
    struct ApplicationProperties
    {
        WindowProperties windowProperties;
        double fps = 0.0;
        bool useVSync = true;
    };
Then in PekanApplication's run() function we'll enable VSync only if there is no target FPS and useVSync is true
    if (fps <= 0.0 && useVSync)
    {
        PekanEngine::getWindow().enableVSync();
    }
and we'll manually wait using FpsLimiter only if there is a target FPS
    if (fps > 0.0)
    {
        fpsLimiter.wait();
    }
Notice, this leaves the possibility of having fps = 0.0 and useVSync = false,
in which case we'll just not limit FPS at all, we'll have maximum FPS, possibly more than monitor's refresh rate.
Okay, looking better now. Works the same as before.

Changed some other minor things as well. For example removed the "friendship" between Window and PekanEngine.
It was needed only so that PekanEngine can access Window's underlying GLFW window in order to initialize ImGui.
Soon we'll move ImGui to its own subsystem so it will be initialized there.
We can just have a getter for the GLFW window
    inline GLFWwindow* getGlfwWindow() { return m_glfwWindow; }
That's fine. Works the same, of course.

With that, Core is cleaned up.
Everything looks perfect. Nothing more to do.
}

{
Let's continue now with TO-DO items, starting with this one
    Add "bool dynamic" parameter to create() functions of all shape types.
    It should do the same as before. I just removed it for a bit while restructuring shape classes.

This "dynamic" parameter of shapes was used to determine whether to use
    BufferDataUsage::DynamicDraw
or
    BufferDataUsage::StaticDraw
for the underlying vertex buffer and index buffer of a shape.
Now that we do Batch Rendering this is no longer valid.
Shapes no longer have "an underlying vertex buffer and index buffer".
Instead Renderer2D manages a batch that has an underlying vertex buffer and index buffer,
and it contains the vertices and indices of all shapes.
So how can we have some of the shapes using DynamicDraw buffers and others using StaticDraw buffers?
Well, we can have 2 separate batches - a DynamicDraw one and a StaticDraw one,
and add DynamicDraw shapes to the DynamicDraw batch and StaticDraw shapes to the StaticDraw batch.
So each shape only needs to know if it's DynamicDraw or StaticDraw and provide API for that to Renderer2D.
Let's add this new member to class Shape
    bool m_isDynamic = true;
and add a "dynamic" parameter to Shape's create() function
    void create(bool dynamic);
Inside of it we'll just set
    m_isDynamic = dynamic;
of course.
Then, inside of each specific shape type, like RectangleShape, we also need a "dynamic" parameter in the create() function
    void create(float width, float height, bool dynamic = true);
and we'll just call Shape::create() passing this "dynamic" parameter along
    Shape::create(dynamic);
Finally, in class Shape we need a getter for this "isValid" state of the shape
    inline bool isDynamic() const { return m_isDynamic; }
That's it from Shape's side.
Then in Renderer2D's side, we need to have 2 separate batches
    static RenderObject s_batchDynamic;
    static RenderObject s_batchStatic;
Together with that, we'll need 2 separate lists of vertices and 2 separate lists of indices
    static std::vector<ShapeVertex> s_verticesDynamic;
    static std::vector<ShapeVertex> s_verticesStatic;
    static std::vector<unsigned> s_indicesDynamic;
    static std::vector<unsigned> s_indicesStatic;
Then in the implementation of Renderer2D's functions, everywhere we use s_batch or s_vertices or s_indices,
we'll have to use both the dynamic and static version.
Interesting part is the render() function.
There we need to check if the given shape is dynamic or static,
and based on that add it to the dynamic batch or the static batch
    if (shape.isDynamic())
    ...
    else
    ...
Also, in the init() function we need to create s_batchDynamic with BufferDataUsage::DynamicDraw
and s_batchStatic with BufferDataUsage::StaticDraw, of course.
That's it.

Tested it by making some of the shapes in Demo06 and Demo07 static and others dynamic.
It always works.

----------
09.07.2025
----------

Now let's refactor this a little bit because it's getting annoying manually managing
2 batches and 2 lists of vertices and 2 lists of indices.
It'd be better to have a class ShapesBatch that
manages a list of vertices and a list of indices
and it does all these things internally.
Then in Renderer2D we'll just have to manage 2 ShapesBatch'es and use their API.
Okay, did it.
Created a class
    ShapesBatch
in 2 new files in Renderer/2D/Shapes/
    ShapesBatch.h
    ShapesBatch.cpp
It consists of a list of vertices and a list of indices
    std::vector<ShapeVertex> m_vertices;
    std::vector<unsigned> m_indices;
and has an underlying render object
    RenderObject m_renderObject;
that is used for actually putting together the vertices and indices into a renderable thing.
It has create() and destroy() functions, of course
    void create(Graphics::BufferDataUsage bufferDataUsage);
    void destroy();
The create() function takes in a
    BufferDataUsage
because the caller needs to specify if they want a "dynamic" batch or a "static" batch (or some other type of batch).
It's basically just the data usage of the underlying vertex buffer and index buffer.
Then we have this function
    void addShape(const Shape& shape);
that can be used to add a shape to the batch, adding its vertices and indices to the vertices and indices lists.
Then we have 2 overloads of a render() function
    void render(const Camera2DPtr& camera);
    void render();
used to render all shapes of the batch - all triangles making up all shapes, at once, with a single draw call.
Finally, we have a function
    void clear();
used to clear the batch - removing all added shapes, removing their vertices and indices from the lists.
That's it.

Now in Renderer2D, instead of having 2 raw RenderObjects, 2 lists of vertices and 2 lists of indices,
we can just have 2 ShapesBatch'es
    static ShapesBatch s_batchDynamic;
    static ShapesBatch s_batchStatic;
The implementation of Renderer2D's functions becomes much simpler.

It works!
Same as before, but code is much nicer.

Considering the TO-DO item done.
}

----------
11.07.2025
----------

{
Next TO-DO item
    Use a 1D texture for batch rendering of 2D shapes to pass colors to shader,
    instead of having a color attribute on each vertex (wasteful for memory)
is a bit of a scary one, but it should be fine.
Let's look at it in step by step.

The general idea is:
1. Remove the "color" vertex attribute from ShapeVertex.
2. Add a "shapeIndex" vertex attribute to ShapeVertex.
3. Set "shapeIndex" vertex attribute to each vertex.
   Do this in ShapesBatch, because it requires knowing how many shapes are already in the batch.
4. Keep track of a list of colors - the colors of all shapes in a ShapesBatch.
5. Upon rendering of ShapesBatch create a 1D texture from the colors, bind it, and set a corresponding uniform in shader
6. In the fragment shader sample the 1D texture based on shapeIndex

Okay, did it.
First had to create a class
    Texture1D
in new files under Graphics/RenderComponents/
    Texture1D.h
    Texture1D.cpp
and rename class Texture to Texture2D.
Texture1D is basically the same as Texture2D but it works on a list of colors, instead of an image

Everything else was fairly straightforward, following the plan above.
Not much to say.

It works now (almost).
Demo07 works perfectly fine.
Demo06 does NOT work if you have too many shapes (40000 doesn't work, 10000 does).
I found out it's because a 1D texture has some upper limit on how many texels it can have,
and apparently 40000 is too many on my GPU.

We can fix this and do this TO-DO item at the same time:
    Think about using more than 1 batch for batch rendering.
    Maybe having some reasonable limit of vertices/indices per batch,
    and when we reach it we can draw the batch and start a new one.

First of all, I'll create this function in RenderState
for getting the max texture size supported on current hardware:
    static int getMaxTextureSize();

Then, of course, in Renderer2D and/or ShapesBatch we'll use this maximum.
We'll not allow the number of colors in the 1D texture to exceed this maximum.
It's even good to not be close to the maximum, let's keep it up to 80% or something like that.
So whenever we reach 80% of the max texture size, we'll render the batch, and start a new one.

In a similar fashion, it's good to have a maximum number of vertices and a maximum number of indices per batch,
and whenever we exceed one of them, we'll render the batch, and start a new one.
I changed ShapesBatch's function
    bool addShape(const Shape& shape);
to return bool, indicating if more shapes can be added after this one,
or batch is full and needs to be rendered.
The way it determined this is based on these 3 constants that I added
    static const float MAX_TEXTURE_SIZE_FRACTION = 0.8f;
    static constexpr size_t MAX_VERTICES_PER_BATCH = 10000;
    static constexpr size_t MAX_INDICES_PER_BATCH = 15000;
and, of course, the maximum texture size allowed - getMaxTextureSize().

Then, in Renderer2D we'll look at addShape()'s return value,
and if it's false, we'll render the batch there and then, clear it, and start a new one.
Not too hard.
Done.

There were some minor bugs but I fixed them.

It works!
Demo06 is even faster now.

Considering the 2 TO-DO items done now.
}

----------
13.07.2025
----------

{
Next TO-DO item is
    Rename classes that have only pure virtual functions with the "I" prefix,
    for example EventListener -> IEventListener

Looked through all classes. There is a single class that is a pure interface
    ISubsystem
and it already begins with "I".
All other classes have some functions that are implementd and/or some data.
So nothing to do here.
Done.
}

{
Next TO-DO item
    Change core architecture so that window is not created by client code,
    or think of something else. Maybe also move all OpenGL and window logic into Renderer instead of Core,
    or better yet create a new module Graphics. Idk, many options, think about it.
    Problem is there isn't a good moment to initialize Renderer2D.
is already done, we should've marked it as done earlier.
We no longer leave window creation to be a responsibility of client code.
Instead we have this virtual function in PekanApplication
    virtual ApplicationProperties getProperties() const { return {}; }
that can be overriden by derived applications to return application properties,
part of which are WindowProperties, and then the actual window creation is done by engine using those properties.
Also, we moved OpenGL code to a separate module called Graphics.
We did NOT move window logic to Renderer, because I thought about it and it doesn't make sense.
The window logic SHOULD be in Core, because a window is not fundamentally about graphics or rendering,
it's just where the application runs. Another big window thing is handling events,
and we wouldn't want Renderer to handle events.
So yeah, window logic remains in Core, not graphics-specific logic but window logic in general,
then graphics-specific logic is in Graphics, and finally Renderer builds on top of Graphics
to provide shapes, cameras, batch rendering, etc.

As for the initialization of Renderer2D, we have a new subsystem architecture,
handled by SubsystemManager - Renderer2D and Graphics are subsystems and are initialized
by the engine AFTER creating the window (because they need the window).

Soo, considering this done.
}

{
Next TO-DO item is
    Why does this function in PekanEngine return a reference and not a const reference?
        inline static Window& getWindow() { return s_window; }

Let's directly try to make it const and see if build will fail.
Yep, it fails, but only for a single reason (well, a few reasons but in one place).
Basically, class PekanApplication needs to access the window in a non-const way,
to be able to close it, enable VSync, etc.
That's fair, PekanApplication should be authorized to do that.
However, it'd be good to NOT allow any other code to act in a non-const way on the window,
so we can change the function to return a const reference
    inline static Window& getWindow() { return s_window; }
and then declare PekanApplication as a friend to PekanEngine
    friend class PekanApplication;
so that in PekanApplication we can directly access
    PekanEngine::s_window
I think this makes more sense.
We grant private access only to PekanApplication, so that only PekanApplication can f with the window.

Okay, looks good.
Works same as before, of course.
Done.
}

{
Next TO-DO item is
    Fix Demo05 and Demo06, shapes are missing
I think I meant Demo04 and Demo05, because Demo06 works fine.

Actually, after recent changes, Demo04 and Demo05 don't work at all.
It's not that shapes are missing. They just crash.

That's chill tho, there's a simple reason - they don't include the Renderer2D subsystem
    PEKAN_INCLUDE_SUBSYSTEM_RENDERER2D;
in their main() functions, and they don't call Renderer2D's beginFrame() and endFrame()
in their scene's render() functions.

Once I did that, I noticed a few more tiny bugs.
Only one worh mentioning is that class Line was not correctly using the camera.
It still had 2 overloads of render() - one with a camera parameter and one without,
but now we're past this, instead we have a single "global" camera set in Renderer2D
and Line needs to use that.
Fixed it, no biggie.

That's it.
Now Demo04 and Demo05 work fine.
Done.
}
